{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rr0ApGDfKtuh",
        "outputId": "2db77daa-c4cc-4db6-8351-eb88ef96d702"
      },
      "outputs": [],
      "source": [
        "\n",
        "# # IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# # TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# # THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# # NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# # ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# # NOTEBOOK.\n",
        "\n",
        "# import os\n",
        "# import sys\n",
        "# from tempfile import NamedTemporaryFile\n",
        "# from urllib.request import urlopen\n",
        "# from urllib.parse import unquote, urlparse\n",
        "# from urllib.error import HTTPError\n",
        "# from zipfile import ZipFile\n",
        "# import tarfile\n",
        "# import shutil\n",
        "\n",
        "# CHUNK_SIZE = 40960\n",
        "# DATA_SOURCE_MAPPING = 'home-credit-credit-risk-model-stability:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-competitions-data%2Fkaggle-v2%2F50160%2F7602123%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240228%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240228T192212Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D4e5dd0fd326c400fe8ccc714a10bef6da44c1517fa65467d6bc60ae9bb9df913c1dceff732cb1b12f0d4cf5b7c32f3c20bf299f52b8b237332f8a795d480e573f354448e605bd754bb643d076696e8a7a80d0304057c6f790eb0312241535c333dcc5251549c110f32cc1c13709ef4f2a4937850ad3d850eb617928c96376eca52d94545e99cbc01130648d04b8bfff0f4537e8ffc3650afe31360b30762ab7619e417ada27e095f7abe73e4d5866d7f5b88ecb805adad5f2c48bafc6f2067ba722a8e3bd87bba8c75ce3e90e8f3ccc69110a679b1cd0e10832e9dc3538a83cbf9824320a50aa2e2a43b50949ef721e48dbc6ab3382d844c0e10d3f1f88e9626'\n",
        "\n",
        "# KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "# KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "# KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "# !umount /kaggle/input/ 2> /dev/null\n",
        "# shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "# os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "# os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "# try:\n",
        "#   os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "# except FileExistsError:\n",
        "#   pass\n",
        "# try:\n",
        "#   os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "# except FileExistsError:\n",
        "#   pass\n",
        "\n",
        "# for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "#     directory, download_url_encoded = data_source_mapping.split(':')\n",
        "#     download_url = unquote(download_url_encoded)\n",
        "#     filename = urlparse(download_url).path\n",
        "#     destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "#     try:\n",
        "#         with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "#             total_length = fileres.headers['content-length']\n",
        "#             print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "#             dl = 0\n",
        "#             data = fileres.read(CHUNK_SIZE)\n",
        "#             while len(data) > 0:\n",
        "#                 dl += len(data)\n",
        "#                 tfile.write(data)\n",
        "#                 done = int(50 * dl / int(total_length))\n",
        "#                 sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "#                 sys.stdout.flush()\n",
        "#                 data = fileres.read(CHUNK_SIZE)\n",
        "#             if filename.endswith('.zip'):\n",
        "#               with ZipFile(tfile) as zfile:\n",
        "#                 zfile.extractall(destination_path)\n",
        "#             else:\n",
        "#               with tarfile.open(tfile.name) as tarfile:\n",
        "#                 tarfile.extractall(destination_path)\n",
        "#             print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "#     except HTTPError as e:\n",
        "#         print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "#         continue\n",
        "#     except OSError as e:\n",
        "#         print(f'Failed to load {download_url} to path {destination_path}')\n",
        "#         continue\n",
        "\n",
        "# print('Data source import complete.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-23T07:49:56.425276Z",
          "iopub.status.busy": "2024-02-23T07:49:56.424841Z",
          "iopub.status.idle": "2024-02-23T07:50:00.415563Z",
          "shell.execute_reply": "2024-02-23T07:50:00.414631Z",
          "shell.execute_reply.started": "2024-02-23T07:49:56.425239Z"
        },
        "id": "58WSemvWKtuj",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# !pip install scikit-learn numpy pandas torch polars scipy\n",
        "\n",
        "\n",
        "import polars as pl\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "import polars.selectors as cs\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import missingno as mn\n",
        "\n",
        "dataPath = \"/home/brett/Desktop/tutorials/kag/ds/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-23T07:50:00.418355Z",
          "iopub.status.busy": "2024-02-23T07:50:00.417876Z",
          "iopub.status.idle": "2024-02-23T07:50:00.431588Z",
          "shell.execute_reply": "2024-02-23T07:50:00.430221Z",
          "shell.execute_reply.started": "2024-02-23T07:50:00.4183Z"
        },
        "id": "eGbPRUpjKtuj",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def set_table_dtypes(df: pl.DataFrame) -> pl.DataFrame:\n",
        "    # implement here all desired dtypes for tables\n",
        "    # the following is just an example\n",
        "    for col in df.columns:\n",
        "        # last letter of column name will help you determine the type\n",
        "        if col[-1] in (\"P\", \"A\"):\n",
        "            df = df.with_columns(pl.col(col).cast(pl.Float64).alias(col))\n",
        "        if col[-1] in (\"D\"):\n",
        "            df = df.with_columns(pl.col(col).cast(pl.Date).alias(col))\n",
        "\n",
        "    return df\n",
        "\n",
        "def convert_strings(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    for col in df.columns:\n",
        "        if df[col].dtype.name in ['object', 'string']:\n",
        "            df[col] = df[col].astype(\"string\").astype('category')\n",
        "            current_categories = df[col].cat.categories\n",
        "            new_categories = current_categories.to_list() + [\"Unknown\"]\n",
        "            new_dtype = pd.CategoricalDtype(categories=new_categories, ordered=True)\n",
        "            df[col] = df[col].astype(new_dtype)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3usGQrpKtuk"
      },
      "source": [
        "# Load feature_def"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-23T07:50:00.434477Z",
          "iopub.status.busy": "2024-02-23T07:50:00.433651Z",
          "iopub.status.idle": "2024-02-23T07:50:00.529708Z",
          "shell.execute_reply": "2024-02-23T07:50:00.528253Z",
          "shell.execute_reply.started": "2024-02-23T07:50:00.434426Z"
        },
        "id": "aWYKqLKzKtuk",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def get_feature_definitions(columns):\n",
        "    return pl.DataFrame({'Variable': columns}).join(\n",
        "        feature_def,\n",
        "        on = 'Variable',\n",
        "        how = 'left',\n",
        "    )\n",
        "\n",
        "feature_def = pl.read_csv(dataPath + \"feature_definitions.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DHjOdF2Ktul"
      },
      "source": [
        "# Load Basetable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-23T07:50:00.533693Z",
          "iopub.status.busy": "2024-02-23T07:50:00.533232Z",
          "iopub.status.idle": "2024-02-23T07:50:01.487871Z",
          "shell.execute_reply": "2024-02-23T07:50:01.486995Z",
          "shell.execute_reply.started": "2024-02-23T07:50:00.533656Z"
        },
        "id": "qpW_kI66Ktul",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# train\n",
        "train_basetable = pl.read_csv(dataPath + \"csv_files/train/train_base.csv\")\n",
        "train_basetable = train_basetable.with_columns(pl.col('date_decision').str.to_date())\n",
        "\n",
        "# test\n",
        "test_basetable = pl.read_csv(dataPath + \"csv_files/test/test_base.csv\")\n",
        "test_basetable = test_basetable.with_columns(pl.col('date_decision').str.to_date())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNNVEqcnKtul"
      },
      "source": [
        "# Check Basetable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-23T07:50:01.490202Z",
          "iopub.status.busy": "2024-02-23T07:50:01.489799Z",
          "iopub.status.idle": "2024-02-23T07:50:01.49461Z",
          "shell.execute_reply": "2024-02-23T07:50:01.49359Z",
          "shell.execute_reply.started": "2024-02-23T07:50:01.490166Z"
        },
        "id": "PvmJS8c-Ktul",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# print(train_basetable.columns)\n",
        "# print('\\n')\n",
        "# print(train_basetable.shape)\n",
        "# print('\\n')\n",
        "# display(train_basetable.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLmoCjIrKtul"
      },
      "source": [
        "# Load depth=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-23T07:50:01.496396Z",
          "iopub.status.busy": "2024-02-23T07:50:01.496003Z",
          "iopub.status.idle": "2024-02-23T07:50:19.259309Z",
          "shell.execute_reply": "2024-02-23T07:50:19.258095Z",
          "shell.execute_reply.started": "2024-02-23T07:50:01.496364Z"
        },
        "id": "x_dj-ygOKtum",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# train\n",
        "train_static = pl.concat(\n",
        "    [\n",
        "        pl.read_csv(dataPath + \"csv_files/train/train_static_0_0.csv\").pipe(set_table_dtypes),\n",
        "        pl.read_csv(dataPath + \"csv_files/train/train_static_0_1.csv\").pipe(set_table_dtypes),\n",
        "    ],\n",
        "    how=\"vertical_relaxed\",\n",
        ")\n",
        "train_static_cb = pl.read_csv(dataPath + \"csv_files/train/train_static_cb_0.csv\").pipe(set_table_dtypes)\n",
        "\n",
        "# test\n",
        "test_static = pl.concat(\n",
        "    [\n",
        "        pl.read_csv(dataPath + \"csv_files/test/test_static_0_0.csv\").pipe(set_table_dtypes),\n",
        "        pl.read_csv(dataPath + \"csv_files/test/test_static_0_1.csv\").pipe(set_table_dtypes),\n",
        "        pl.read_csv(dataPath + \"csv_files/test/test_static_0_2.csv\").pipe(set_table_dtypes),\n",
        "    ],\n",
        "    how=\"vertical_relaxed\",\n",
        ")\n",
        "test_static_cb = pl.read_csv(dataPath + \"csv_files/test/test_static_cb_0.csv\").pipe(set_table_dtypes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hi-jqRJ2Ktum"
      },
      "source": [
        "# Check depth=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-23T07:50:19.261185Z",
          "iopub.status.busy": "2024-02-23T07:50:19.260754Z",
          "iopub.status.idle": "2024-02-23T07:50:19.26836Z",
          "shell.execute_reply": "2024-02-23T07:50:19.266079Z",
          "shell.execute_reply.started": "2024-02-23T07:50:19.261145Z"
        },
        "id": "K8e7HwwIKtum",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# print(train_static.columns)\n",
        "# print('\\n')\n",
        "# print(train_static.shape)\n",
        "# print('\\n')\n",
        "# display(train_static.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-23T07:50:19.269922Z",
          "iopub.status.busy": "2024-02-23T07:50:19.269527Z",
          "iopub.status.idle": "2024-02-23T07:50:19.278502Z",
          "shell.execute_reply": "2024-02-23T07:50:19.277304Z",
          "shell.execute_reply.started": "2024-02-23T07:50:19.269883Z"
        },
        "id": "JBT_m_o5Ktum",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# print(train_static_cb.columns)\n",
        "# print('\\n')\n",
        "# print(train_static_cb.shape)\n",
        "# print('\\n')\n",
        "# display(train_static_cb.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjcS2RKDKtum"
      },
      "source": [
        "# depth=1에 해당하는 테이블\n",
        "\n",
        "Internal file\n",
        "> `applprev_1` `debitcard_1` `deposit_1` `other_1` `person_1`\n",
        "\n",
        "External file\n",
        "> `credit_bureau_b_1`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-23T07:50:19.280343Z",
          "iopub.status.busy": "2024-02-23T07:50:19.279929Z",
          "iopub.status.idle": "2024-02-23T07:50:47.056951Z",
          "shell.execute_reply": "2024-02-23T07:50:47.052874Z",
          "shell.execute_reply.started": "2024-02-23T07:50:19.280303Z"
        },
        "id": "ZwaD7eN1Ktum",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "## applprev_1\n",
        "# train\n",
        "train_applprev_1 = pl.concat(\n",
        "    [\n",
        "        pl.read_csv(dataPath + \"csv_files/train/train_applprev_1_0.csv\").pipe(set_table_dtypes),\n",
        "        pl.read_csv(dataPath + 'csv_files/train/train_applprev_1_1.csv').pipe(set_table_dtypes),\n",
        "    ],\n",
        "    how='vertical_relaxed',\n",
        ")\n",
        "\n",
        "# test\n",
        "test_applprev_1 = pl.concat(\n",
        "    [\n",
        "        pl.read_csv(dataPath + \"csv_files/test/test_applprev_1_0.csv\").pipe(set_table_dtypes),\n",
        "        pl.read_csv(dataPath + 'csv_files/test/test_applprev_1_1.csv').pipe(set_table_dtypes),\n",
        "        pl.read_csv(dataPath + 'csv_files/test/test_applprev_1_2.csv').pipe(set_table_dtypes),\n",
        "    ],\n",
        "    how='vertical_relaxed',\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-23T07:50:47.074609Z",
          "iopub.status.busy": "2024-02-23T07:50:47.072454Z",
          "iopub.status.idle": "2024-02-23T07:51:02.371301Z",
          "shell.execute_reply": "2024-02-23T07:51:02.367136Z",
          "shell.execute_reply.started": "2024-02-23T07:50:47.07448Z"
        },
        "id": "RgRQul2XKtum",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# train\n",
        "train_debitcard_1 = pl.read_csv(dataPath + \"csv_files/train/train_debitcard_1.csv\").pipe(set_table_dtypes)\n",
        "train_deposit_1 = pl.read_csv(dataPath + \"csv_files/train/train_deposit_1.csv\").pipe(set_table_dtypes)\n",
        "train_other_1 = pl.read_csv(dataPath + 'csv_files/train/train_other_1.csv').pipe(set_table_dtypes)\n",
        "train_person_1 = pl.read_csv(dataPath + \"csv_files/train/train_person_1.csv\").pipe(set_table_dtypes)\n",
        "train_credit_bureau_b_1 = pl.read_csv(dataPath + 'csv_files/train/train_credit_bureau_b_1.csv').pipe(set_table_dtypes)\n",
        "train_tax_a = pl.read_csv(dataPath + \"csv_files/train/train_tax_registry_a_1.csv\").pipe(set_table_dtypes)\n",
        "\n",
        "# test\n",
        "test_debitcard_1 = pl.read_csv(dataPath + \"csv_files/test/test_debitcard_1.csv\").pipe(set_table_dtypes)\n",
        "test_deposit_1 = pl.read_csv(dataPath + \"csv_files/test/test_deposit_1.csv\").pipe(set_table_dtypes)\n",
        "test_other_1 = pl.read_csv(dataPath + 'csv_files/test/test_other_1.csv').pipe(set_table_dtypes)\n",
        "test_person_1 = pl.read_csv(dataPath + \"csv_files/test/test_person_1.csv\").pipe(set_table_dtypes)\n",
        "test_credit_bureau_b_1 = pl.read_csv(dataPath + 'csv_files/test/test_credit_bureau_b_1.csv').pipe(set_table_dtypes)\n",
        "test_tax_a = pl.read_csv(dataPath + \"csv_files/test/test_tax_registry_a_1.csv\").pipe(set_table_dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-02-23T07:51:02.375844Z",
          "iopub.status.busy": "2024-02-23T07:51:02.375462Z",
          "iopub.status.idle": "2024-02-23T07:51:02.397457Z",
          "shell.execute_reply": "2024-02-23T07:51:02.39337Z",
          "shell.execute_reply.started": "2024-02-23T07:51:02.375812Z"
        },
        "id": "xMTt6q94Ktum",
        "outputId": "a9901694-1344-4eaa-9d33-517c10f0011d",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(6525979, 41) (157302, 6) (145086, 5) (51109, 7) (2973991, 37) (85791, 45) (3275770, 5)\n"
          ]
        }
      ],
      "source": [
        "print(\n",
        "    train_applprev_1.shape,\n",
        "    train_debitcard_1.shape,\n",
        "    train_deposit_1.shape,\n",
        "    train_other_1.shape,\n",
        "    train_person_1.shape,\n",
        "    train_credit_bureau_b_1.shape,\n",
        "    train_tax_a.shape,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7i0mLLSQKtun"
      },
      "source": [
        "# Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-23T07:51:02.407093Z",
          "iopub.status.busy": "2024-02-23T07:51:02.404875Z",
          "iopub.status.idle": "2024-02-23T07:51:05.128997Z",
          "shell.execute_reply": "2024-02-23T07:51:05.126056Z",
          "shell.execute_reply.started": "2024-02-23T07:51:02.406848Z"
        },
        "id": "pcge4WLWKtun",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "## applprev_1\n",
        "app_cols = [\n",
        "    'case_id',\n",
        "    'maxdpdtolerance_577P',\n",
        "    'employedfrom_700D',\n",
        "    'pmtnum_8L',\n",
        "    'creationdate_885D',\n",
        "    'credamount_590A',\n",
        "    'mainoccupationinc_437A',\n",
        "    'firstnonzeroinstldate_307D',\n",
        "    'dtlastpmtallstes_3545839D',\n",
        "    'annuity_853A',\n",
        "    'familystate_726L',\n",
        "    'approvaldate_319D',\n",
        "    'district_544M',\n",
        "    'dateactivated_425D',\n",
        "    'education_1138M',\n",
        "]\n",
        "\n",
        "# train\n",
        "train_applprev_1 = train_applprev_1[app_cols]\n",
        "train_applprev_1 = train_applprev_1.unique(subset='case_id', keep='first')\n",
        "\n",
        "# test\n",
        "test_applprev_1 = test_applprev_1[app_cols]\n",
        "test_applprev_1 = test_applprev_1.unique(subset='case_id', keep='first')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-02-23T07:51:05.136811Z",
          "iopub.status.busy": "2024-02-23T07:51:05.135297Z",
          "iopub.status.idle": "2024-02-23T07:51:15.882136Z",
          "shell.execute_reply": "2024-02-23T07:51:15.881178Z",
          "shell.execute_reply.started": "2024-02-23T07:51:05.136631Z"
        },
        "id": "wKYZPNXGKtun",
        "outputId": "63d6f250-b208-4fe3-f78b-f0af9a7e934d",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['actualdpdtolerance_344P', 'amtinstpaidbefduel24m_4187115A', 'annuity_780A', 'annuitynextmonth_57A', 'applicationcnt_361L', 'applications30d_658L', 'applicationscnt_1086L', 'applicationscnt_464L', 'applicationscnt_629L', 'applicationscnt_867L', 'avgdbddpdlast24m_3658932P', 'avgdbddpdlast3m_4187120P', 'avgdbdtollast24m_4525197P', 'avgdpdtolclosure24_3658938P', 'avginstallast24m_3658937A', 'avglnamtstart24m_4525187A', 'avgmaxdpdlast9m_3716943P', 'avgoutstandbalancel6m_4187114A', 'avgpmtlast12m_4525200A', 'bankacctype_710L', 'cardtype_51L', 'clientscnt12m_3712952L', 'clientscnt3m_3712950L', 'clientscnt6m_3712949L', 'clientscnt_100L', 'clientscnt_1022L', 'clientscnt_1071L', 'clientscnt_1130L', 'clientscnt_136L', 'clientscnt_157L', 'clientscnt_257L', 'clientscnt_304L', 'clientscnt_360L', 'clientscnt_493L', 'clientscnt_533L', 'clientscnt_887L', 'clientscnt_946L', 'cntincpaycont9m_3716944L', 'cntpmts24_3658933L', 'commnoinclast6m_3546845L', 'credamount_770A', 'credtype_322L', 'currdebt_22A', 'currdebtcredtyperange_828A', 'datefirstoffer_1144D', 'datelastinstal40dpd_247D', 'datelastunpaid_3546854D', 'daysoverduetolerancedd_3976961L', 'deferredmnthsnum_166L', 'disbursedcredamount_1113A', 'disbursementtype_67L', 'downpmt_116A', 'dtlastpmtallstes_4499206D', 'eir_270L', 'equalitydataagreement_891L', 'equalityempfrom_62L', 'firstclxcampaign_1125D', 'firstdatedue_489D', 'homephncnt_628L', 'inittransactionamount_650A', 'inittransactioncode_186L', 'interestrate_311L', 'interestrategrace_34L', 'isbidproduct_1095L', 'isbidproductrequest_292L', 'isdebitcard_729L', 'lastactivateddate_801D', 'lastapplicationdate_877D', 'lastapprcommoditycat_1041M', 'lastapprcommoditytypec_5251766M', 'lastapprcredamount_781A', 'lastapprdate_640D', 'lastcancelreason_561M', 'lastdelinqdate_224D', 'lastdependentsnum_448L', 'lastotherinc_902A', 'lastotherlnsexpense_631A', 'lastrejectcommoditycat_161M', 'lastrejectcommodtypec_5251769M', 'lastrejectcredamount_222A', 'lastrejectdate_50D', 'lastrejectreason_759M', 'lastrejectreasonclient_4145040M', 'lastrepayingdate_696D', 'lastst_736L', 'maininc_215A', 'mastercontrelectronic_519L', 'mastercontrexist_109L', 'maxannuity_159A', 'maxannuity_4075009A', 'maxdbddpdlast1m_3658939P', 'maxdbddpdtollast12m_3658940P', 'maxdbddpdtollast6m_4187119P', 'maxdebt4_972A', 'maxdpdfrom6mto36m_3546853P', 'maxdpdinstldate_3546855D', 'maxdpdinstlnum_3546846P', 'maxdpdlast12m_727P', 'maxdpdlast24m_143P', 'maxdpdlast3m_392P', 'maxdpdlast6m_474P', 'maxdpdlast9m_1059P', 'maxdpdtolerance_374P', 'maxinstallast24m_3658928A', 'maxlnamtstart6m_4525199A', 'maxoutstandbalancel12m_4187113A', 'maxpmtlast3m_4525190A', 'mindbddpdlast24m_3658935P', 'mindbdtollast24m_4525191P', 'mobilephncnt_593L', 'monthsannuity_845L', 'numactivecreds_622L', 'numactivecredschannel_414L', 'numactiverelcontr_750L', 'numcontrs3months_479L', 'numincomingpmts_3546848L', 'numinstlallpaidearly3d_817L', 'numinstls_657L', 'numinstlsallpaid_934L', 'numinstlswithdpd10_728L', 'numinstlswithdpd5_4187116L', 'numinstlswithoutdpd_562L', 'numinstmatpaidtearly2d_4499204L', 'numinstpaid_4499208L', 'numinstpaidearly3d_3546850L', 'numinstpaidearly3dest_4493216L', 'numinstpaidearly5d_1087L', 'numinstpaidearly5dest_4493211L', 'numinstpaidearly5dobd_4499205L', 'numinstpaidearly_338L', 'numinstpaidearlyest_4493214L', 'numinstpaidlastcontr_4325080L', 'numinstpaidlate1d_3546852L', 'numinstregularpaid_973L', 'numinstregularpaidest_4493210L', 'numinsttopaygr_769L', 'numinsttopaygrest_4493213L', 'numinstunpaidmax_3546851L', 'numinstunpaidmaxest_4493212L', 'numnotactivated_1143L', 'numpmtchanneldd_318L', 'numrejects9m_859L', 'opencred_647L', 'paytype1st_925L', 'paytype_783L', 'payvacationpostpone_4187118D', 'pctinstlsallpaidearl3d_427L', 'pctinstlsallpaidlat10d_839L', 'pctinstlsallpaidlate1d_3546856L', 'pctinstlsallpaidlate4d_3546849L', 'pctinstlsallpaidlate6d_3546844L', 'pmtnum_254L', 'posfpd10lastmonth_333P', 'posfpd30lastmonth_3976960P', 'posfstqpd30lastmonth_3976962P', 'previouscontdistrict_112M', 'price_1097A', 'sellerplacecnt_915L', 'sellerplacescnt_216L', 'sumoutstandtotal_3546847A', 'sumoutstandtotalest_4493215A', 'totaldebt_9A', 'totalsettled_863A', 'totinstallast1m_4525188A', 'twobodfilling_608L', 'typesuite_864L', 'validfrom_1069D']\n",
            "['contractssum_5085716L', 'days120_123L', 'days180_256L', 'days30_165L', 'days360_512L', 'days90_310L', 'description_5085714M', 'education_1103M', 'education_88M', 'firstquarter_103L', 'for3years_128L', 'for3years_504L', 'for3years_584L', 'formonth_118L', 'formonth_206L', 'formonth_535L', 'forquarter_1017L', 'forquarter_462L', 'forquarter_634L', 'fortoday_1092L', 'forweek_1077L', 'forweek_528L', 'forweek_601L', 'foryear_618L', 'foryear_818L', 'foryear_850L', 'fourthquarter_440L', 'maritalst_385M', 'maritalst_893M', 'numberofqueries_373L', 'pmtaverage_3A', 'pmtaverage_4527227A', 'pmtaverage_4955615A', 'pmtcount_4527229L', 'pmtcount_4955617L', 'pmtcount_693L', 'pmtscount_423L', 'pmtssum_45A', 'requesttype_4525192L', 'riskassesment_302T', 'riskassesment_940T', 'secondquarter_766L', 'thirdquarter_1082L']\n"
          ]
        }
      ],
      "source": [
        "## case_id를 기준으로 그룹화한 후, aggregation functions\n",
        "\n",
        "# applprev_1\n",
        "train_applprev_1_feats_1 = train_applprev_1.group_by(\"case_id\").agg(\n",
        "    pl.col(\"pmtnum_8L\").max().alias(\"max_pmtnum_8L\"))\n",
        "\n",
        "# debitcard_1\n",
        "train_debitcard_1_feats = train_debitcard_1.group_by(\"case_id\").agg(\n",
        "    # 체크카드의 평균 잔액의 최대값\n",
        "    pl.col('last180dayaveragebalance_704A').max().alias('max_last180dayaveragebalance_704A'),\n",
        "    # 180일간의 체크카드 거래액의 최대값\n",
        "    pl.col('last180dayturnover_1134A').max().alias('max_last180dayturnover_1134A'),\n",
        "    # 30일간의 체크카드 거래액의 최대값\n",
        "    pl.col(\"last30dayturnover_651A\").max().alias(\"max_last30dayturnover_651A\"),\n",
        ").sort(by='case_id')\n",
        "\n",
        "# deposit_1\n",
        "train_deposit_1_feats = train_deposit_1.group_by('case_id').agg(\n",
        "    # 예/적금의 합계\n",
        "    pl.sum('amount_416A').alias('sum_amount_416A'),\n",
        ").sort(by='case_id')\n",
        "\n",
        "# other_1\n",
        "train_other_1_feats = train_other_1.group_by(\"case_id\").agg(\n",
        "    # 입금액의 합계\n",
        "    pl.col(\"amtdepositincoming_4809444A\").sum().alias(\"sum_amtdepositincoming_4809444A\"),\n",
        "    # 출금액의 합계\n",
        "    pl.col(\"amtdepositoutgoing_4809442A\").sum().alias(\"sum_amtdepositoutgoing_4809442A\"),\n",
        ").sort(by='case_id')\n",
        "\n",
        "# person_1\n",
        "train_person_1_feats_1 = train_person_1.select(['case_id', 'incometype_1044T','num_group1']).filter(\n",
        "    pl.col('num_group1') == 0\n",
        ").drop('num_group1')\n",
        "\n",
        "train_person_1_feats_2 = train_person_1.select(['case_id', 'language1_981M','num_group1']).filter(\n",
        "    pl.col('num_group1') == 0\n",
        ").drop('num_group1')\n",
        "\n",
        "# credit_bureau_b_1\n",
        "train_credit_bureau_b_1_feats = train_credit_bureau_b_1.group_by('case_id').agg(\n",
        "    pl.col('installmentamount_644A').mean().alias('mean_installmentamount_644A'),\n",
        "    pl.col('maxdebtpduevalodued_3940955A').mean().alias('mean_maxdebtpduevalodued_3940955A'),\n",
        "    pl.col('overdueamountmax_950A').mean().alias('mean_overdueamountmax_950A'),\n",
        "    pl.col('residualamount_1093A').mean().alias('mean_residualamount_1093A'),\n",
        ").sort(by='case_id')\n",
        "\n",
        "\n",
        "## ⭐키워드 6가지 중 어떤걸 선택할지?\n",
        "selected_static_cols = []\n",
        "for col in train_static.columns:\n",
        "    if col[-1] in (\"A\", \"D\", \"M\", 'P', 'T', \"L\"):\n",
        "        selected_static_cols.append(col)\n",
        "print(selected_static_cols)\n",
        "\n",
        "# static_cb_0에서는 D를 사용할 필요가 없어보임.\n",
        "selected_static_cb_cols = []\n",
        "for col in train_static_cb.columns:\n",
        "    if col[-1] in (\"A\", \"M\", 'P', 'T', 'L'):\n",
        "        selected_static_cb_cols.append(col)\n",
        "print(selected_static_cb_cols)\n",
        "\n",
        "\n",
        "## Join\n",
        "data = train_basetable.join(\n",
        "    train_static.select([\"case_id\"]+selected_static_cols), how=\"left\", on=\"case_id\"\n",
        ").join(\n",
        "    train_static_cb.select([\"case_id\"]+selected_static_cb_cols), how=\"left\", on=\"case_id\"\n",
        "# ).join(\n",
        "#     train_applprev_1, how=\"left\", on=\"case_id\"\n",
        ").join(\n",
        "    train_debitcard_1_feats, how=\"left\", on=\"case_id\"\n",
        ").join(\n",
        "    train_deposit_1_feats, how=\"left\", on=\"case_id\"\n",
        ").join(\n",
        "    train_other_1_feats, how=\"left\", on=\"case_id\"\n",
        ").join(\n",
        "    train_person_1_feats_1, how=\"left\", on=\"case_id\"\n",
        ").join(\n",
        "    train_person_1_feats_2, how=\"left\", on=\"case_id\"\n",
        ").join(\n",
        "    train_credit_bureau_b_1_feats, how=\"left\", on=\"case_id\"\n",
        ").join(\n",
        "    train_applprev_1_feats_1, how=\"left\", on=\"case_id\"\n",
        "# ).join(\n",
        "#     train_tax_a, how=\"left\", on=\"case_id\"\n",
        ")\n",
        "\n",
        "\n",
        "## num_group1이 0인 경우 대출을 신청한 사람\n",
        "# train_debitcard_1_feats_2 = train_debitcard_1.select([\"case_id\", \"num_group1\"]).filter(\n",
        "#     pl.col(\"num_group1\") == 0 # 필터링 후\n",
        "# ).drop(\"num_group1\") # 컬럼 삭제"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-23T07:51:15.883498Z",
          "iopub.status.busy": "2024-02-23T07:51:15.883174Z",
          "iopub.status.idle": "2024-02-23T07:51:15.89611Z",
          "shell.execute_reply": "2024-02-23T07:51:15.894719Z",
          "shell.execute_reply.started": "2024-02-23T07:51:15.883468Z"
        },
        "id": "8EAy-GGrKtun",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def calculate_date_difference(df):\n",
        "    for col in df.columns:\n",
        "        if col[-1] == \"D\":\n",
        "            df = df.with_columns(\n",
        "                (pl.col('date_decision') - pl.col(col)).cast(pl.Int64).alias(col)\n",
        "            )\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-23T07:51:15.899517Z",
          "iopub.status.busy": "2024-02-23T07:51:15.898351Z",
          "iopub.status.idle": "2024-02-23T07:51:17.110845Z",
          "shell.execute_reply": "2024-02-23T07:51:17.109499Z",
          "shell.execute_reply.started": "2024-02-23T07:51:15.899467Z"
        },
        "id": "rWUHUa1oKtun",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "data = calculate_date_difference(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-23T07:51:17.11309Z",
          "iopub.status.busy": "2024-02-23T07:51:17.112618Z",
          "iopub.status.idle": "2024-02-23T07:51:17.152904Z",
          "shell.execute_reply": "2024-02-23T07:51:17.151445Z",
          "shell.execute_reply.started": "2024-02-23T07:51:17.113046Z"
        },
        "id": "eeGml00kKtun",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "test_applprev_1_feats_1 = test_applprev_1.group_by(\"case_id\").agg(\n",
        "    pl.col(\"pmtnum_8L\").max().alias(\"max_pmtnum_8L\"))\n",
        "\n",
        "test_debitcard_1_feats = test_debitcard_1.group_by(\"case_id\").agg(\n",
        "    pl.col('last180dayaveragebalance_704A').max().alias('max_last180dayaveragebalance_704A'),\n",
        "    pl.col('last180dayturnover_1134A').max().alias('max_last180dayturnover_1134A'),\n",
        "    pl.col(\"last30dayturnover_651A\").max().alias(\"max_last30dayturnover_651A\"),\n",
        ").sort(by='case_id')\n",
        "\n",
        "test_deposit_1_feats = test_deposit_1.group_by('case_id').agg(\n",
        "    pl.sum('amount_416A').alias('sum_amount_416A')\n",
        ").sort(by='case_id')\n",
        "\n",
        "test_other_1_feats = test_other_1.group_by(\"case_id\").agg(\n",
        "    pl.col(\"amtdepositincoming_4809444A\").sum().alias(\"sum_amtdepositincoming_4809444A\"),\n",
        "    pl.col(\"amtdepositoutgoing_4809442A\").sum().alias(\"sum_amtdepositoutgoing_4809442A\"),\n",
        ").sort(by='case_id')\n",
        "\n",
        "test_person_1_feats_1 = test_person_1.select(['case_id', 'incometype_1044T','num_group1']).filter(\n",
        "    pl.col('num_group1') == 0\n",
        ").drop('num_group1')\n",
        "\n",
        "test_person_1_feats_2 = test_person_1.select(['case_id', 'language1_981M','num_group1']).filter(\n",
        "    pl.col('num_group1') == 0\n",
        ").drop('num_group1')\n",
        "\n",
        "test_credit_bureau_b_1_feats = test_credit_bureau_b_1.group_by('case_id').agg(\n",
        "    pl.col('installmentamount_644A').mean().alias('mean_installmentamount_644A'),\n",
        "    pl.col('maxdebtpduevalodued_3940955A').mean().alias('mean_maxdebtpduevalodued_3940955A'),\n",
        "    pl.col('overdueamountmax_950A').mean().alias('mean_overdueamountmax_950A'),\n",
        "    pl.col('residualamount_1093A').mean().alias('mean_residualamount_1093A'),\n",
        ").sort(by='case_id')\n",
        "\n",
        "\n",
        "data_submission = test_basetable.join(\n",
        "    test_static.select([\"case_id\"]+selected_static_cols), how=\"left\", on=\"case_id\"\n",
        ").join(\n",
        "    test_static_cb.select([\"case_id\"]+selected_static_cb_cols), how=\"left\", on=\"case_id\"\n",
        "# ).join(\n",
        "#     test_applprev_1, how=\"left\", on=\"case_id\"\n",
        ").join(\n",
        "    test_debitcard_1_feats, how=\"left\", on=\"case_id\"\n",
        ").join(\n",
        "    test_deposit_1_feats, how=\"left\", on=\"case_id\"\n",
        ").join(\n",
        "    test_other_1_feats, how=\"left\", on=\"case_id\"\n",
        ").join(\n",
        "    test_person_1_feats_1, how=\"left\", on=\"case_id\"\n",
        ").join(\n",
        "    test_person_1_feats_2, how=\"left\", on=\"case_id\"\n",
        ").join(\n",
        "    test_credit_bureau_b_1_feats, how=\"left\", on=\"case_id\"\n",
        ").join(\n",
        "    test_applprev_1_feats_1, how=\"left\", on=\"case_id\"\n",
        "# ).join(\n",
        "#     test_tax_a, how=\"left\", on=\"case_id\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-23T07:51:17.157492Z",
          "iopub.status.busy": "2024-02-23T07:51:17.157077Z",
          "iopub.status.idle": "2024-02-23T07:51:17.170923Z",
          "shell.execute_reply": "2024-02-23T07:51:17.169248Z",
          "shell.execute_reply.started": "2024-02-23T07:51:17.157454Z"
        },
        "id": "4P17Yko6Ktuo",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "data_submission = calculate_date_difference(data_submission)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-02-23T07:51:17.17384Z",
          "iopub.status.busy": "2024-02-23T07:51:17.172997Z",
          "iopub.status.idle": "2024-02-23T07:51:48.833442Z",
          "shell.execute_reply": "2024-02-23T07:51:48.831595Z",
          "shell.execute_reply.started": "2024-02-23T07:51:17.173764Z"
        },
        "id": "ZOAsYgqcKtuo",
        "outputId": "36c61b47-8be2-44ac-b9c2-e0c2abc35fa5",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['actualdpdtolerance_344P', 'amtinstpaidbefduel24m_4187115A', 'annuity_780A', 'annuitynextmonth_57A', 'applicationcnt_361L', 'applications30d_658L', 'applicationscnt_1086L', 'applicationscnt_464L', 'applicationscnt_629L', 'applicationscnt_867L', 'avgdbddpdlast24m_3658932P', 'avgdbddpdlast3m_4187120P', 'avgdbdtollast24m_4525197P', 'avgdpdtolclosure24_3658938P', 'avginstallast24m_3658937A', 'avglnamtstart24m_4525187A', 'avgmaxdpdlast9m_3716943P', 'avgoutstandbalancel6m_4187114A', 'avgpmtlast12m_4525200A', 'bankacctype_710L', 'cardtype_51L', 'clientscnt12m_3712952L', 'clientscnt3m_3712950L', 'clientscnt6m_3712949L', 'clientscnt_100L', 'clientscnt_1022L', 'clientscnt_1071L', 'clientscnt_1130L', 'clientscnt_136L', 'clientscnt_157L', 'clientscnt_257L', 'clientscnt_304L', 'clientscnt_360L', 'clientscnt_493L', 'clientscnt_533L', 'clientscnt_887L', 'clientscnt_946L', 'cntincpaycont9m_3716944L', 'cntpmts24_3658933L', 'commnoinclast6m_3546845L', 'credamount_770A', 'credtype_322L', 'currdebt_22A', 'currdebtcredtyperange_828A', 'datefirstoffer_1144D', 'datelastinstal40dpd_247D', 'datelastunpaid_3546854D', 'daysoverduetolerancedd_3976961L', 'deferredmnthsnum_166L', 'disbursedcredamount_1113A', 'disbursementtype_67L', 'downpmt_116A', 'dtlastpmtallstes_4499206D', 'eir_270L', 'equalitydataagreement_891L', 'equalityempfrom_62L', 'firstclxcampaign_1125D', 'firstdatedue_489D', 'homephncnt_628L', 'inittransactionamount_650A', 'inittransactioncode_186L', 'interestrate_311L', 'interestrategrace_34L', 'isbidproduct_1095L', 'isbidproductrequest_292L', 'isdebitcard_729L', 'lastactivateddate_801D', 'lastapplicationdate_877D', 'lastapprcommoditycat_1041M', 'lastapprcommoditytypec_5251766M', 'lastapprcredamount_781A', 'lastapprdate_640D', 'lastcancelreason_561M', 'lastdelinqdate_224D', 'lastdependentsnum_448L', 'lastotherinc_902A', 'lastotherlnsexpense_631A', 'lastrejectcommoditycat_161M', 'lastrejectcommodtypec_5251769M', 'lastrejectcredamount_222A', 'lastrejectdate_50D', 'lastrejectreason_759M', 'lastrejectreasonclient_4145040M', 'lastrepayingdate_696D', 'lastst_736L', 'maininc_215A', 'mastercontrelectronic_519L', 'mastercontrexist_109L', 'maxannuity_159A', 'maxannuity_4075009A', 'maxdbddpdlast1m_3658939P', 'maxdbddpdtollast12m_3658940P', 'maxdbddpdtollast6m_4187119P', 'maxdebt4_972A', 'maxdpdfrom6mto36m_3546853P', 'maxdpdinstldate_3546855D', 'maxdpdinstlnum_3546846P', 'maxdpdlast12m_727P', 'maxdpdlast24m_143P', 'maxdpdlast3m_392P', 'maxdpdlast6m_474P', 'maxdpdlast9m_1059P', 'maxdpdtolerance_374P', 'maxinstallast24m_3658928A', 'maxlnamtstart6m_4525199A', 'maxoutstandbalancel12m_4187113A', 'maxpmtlast3m_4525190A', 'mindbddpdlast24m_3658935P', 'mindbdtollast24m_4525191P', 'mobilephncnt_593L', 'monthsannuity_845L', 'numactivecreds_622L', 'numactivecredschannel_414L', 'numactiverelcontr_750L', 'numcontrs3months_479L', 'numincomingpmts_3546848L', 'numinstlallpaidearly3d_817L', 'numinstls_657L', 'numinstlsallpaid_934L', 'numinstlswithdpd10_728L', 'numinstlswithdpd5_4187116L', 'numinstlswithoutdpd_562L', 'numinstmatpaidtearly2d_4499204L', 'numinstpaid_4499208L', 'numinstpaidearly3d_3546850L', 'numinstpaidearly3dest_4493216L', 'numinstpaidearly5d_1087L', 'numinstpaidearly5dest_4493211L', 'numinstpaidearly5dobd_4499205L', 'numinstpaidearly_338L', 'numinstpaidearlyest_4493214L', 'numinstpaidlastcontr_4325080L', 'numinstpaidlate1d_3546852L', 'numinstregularpaid_973L', 'numinstregularpaidest_4493210L', 'numinsttopaygr_769L', 'numinsttopaygrest_4493213L', 'numinstunpaidmax_3546851L', 'numinstunpaidmaxest_4493212L', 'numnotactivated_1143L', 'numpmtchanneldd_318L', 'numrejects9m_859L', 'opencred_647L', 'paytype1st_925L', 'paytype_783L', 'payvacationpostpone_4187118D', 'pctinstlsallpaidearl3d_427L', 'pctinstlsallpaidlat10d_839L', 'pctinstlsallpaidlate1d_3546856L', 'pctinstlsallpaidlate4d_3546849L', 'pctinstlsallpaidlate6d_3546844L', 'pmtnum_254L', 'posfpd10lastmonth_333P', 'posfpd30lastmonth_3976960P', 'posfstqpd30lastmonth_3976962P', 'previouscontdistrict_112M', 'price_1097A', 'sellerplacecnt_915L', 'sellerplacescnt_216L', 'sumoutstandtotal_3546847A', 'sumoutstandtotalest_4493215A', 'totaldebt_9A', 'totalsettled_863A', 'totinstallast1m_4525188A', 'twobodfilling_608L', 'typesuite_864L', 'validfrom_1069D', 'contractssum_5085716L', 'days120_123L', 'days180_256L', 'days30_165L', 'days360_512L', 'days90_310L', 'description_5085714M', 'education_1103M', 'education_88M', 'firstquarter_103L', 'for3years_128L', 'for3years_504L', 'for3years_584L', 'formonth_118L', 'formonth_206L', 'formonth_535L', 'forquarter_1017L', 'forquarter_462L', 'forquarter_634L', 'fortoday_1092L', 'forweek_1077L', 'forweek_528L', 'forweek_601L', 'foryear_618L', 'foryear_818L', 'foryear_850L', 'fourthquarter_440L', 'maritalst_385M', 'maritalst_893M', 'numberofqueries_373L', 'pmtaverage_3A', 'pmtaverage_4527227A', 'pmtaverage_4955615A', 'pmtcount_4527229L', 'pmtcount_4955617L', 'pmtcount_693L', 'pmtscount_423L', 'pmtssum_45A', 'requesttype_4525192L', 'riskassesment_302T', 'riskassesment_940T', 'secondquarter_766L', 'thirdquarter_1082L', 'max_last180dayaveragebalance_704A', 'max_last180dayturnover_1134A', 'max_last30dayturnover_651A', 'sum_amount_416A', 'sum_amtdepositincoming_4809444A', 'sum_amtdepositoutgoing_4809442A', 'incometype_1044T', 'language1_981M', 'mean_installmentamount_644A', 'mean_maxdebtpduevalodued_3940955A', 'mean_overdueamountmax_950A', 'mean_residualamount_1093A', 'max_pmtnum_8L']\n"
          ]
        }
      ],
      "source": [
        "# !conda install pyarrow -y \n",
        "\n",
        "case_ids = data[\"case_id\"].unique().shuffle(seed=1)\n",
        "case_ids_train, case_ids_test = train_test_split(case_ids, train_size=0.6, random_state=1)\n",
        "case_ids_valid, case_ids_test = train_test_split(case_ids_test, train_size=0.5, random_state=1)\n",
        "\n",
        "cols_pred = []\n",
        "for col in data.columns:\n",
        "    if col[-1].isupper() and col[:-1].islower():\n",
        "        cols_pred.append(col)\n",
        "\n",
        "print(cols_pred)\n",
        "\n",
        "def from_polars_to_pandas(case_ids: pl.DataFrame) -> pl.DataFrame:\n",
        "    return (\n",
        "        data.filter(pl.col(\"case_id\").is_in(case_ids))[[\"case_id\", \"WEEK_NUM\", \"target\"]].to_pandas(),\n",
        "        data.filter(pl.col(\"case_id\").is_in(case_ids))[cols_pred].to_pandas(),\n",
        "        data.filter(pl.col(\"case_id\").is_in(case_ids))[\"target\"].to_pandas()\n",
        "    )\n",
        "\n",
        "base_train, X_train, y_train = from_polars_to_pandas(case_ids_train)\n",
        "base_valid, X_valid, y_valid = from_polars_to_pandas(case_ids_valid)\n",
        "base_test, X_test, y_test = from_polars_to_pandas(case_ids_test)\n",
        "\n",
        "for df in [X_train, X_valid, X_test]:\n",
        "    df = convert_strings(df)\n",
        "\n",
        "print('\\n')\n",
        "print(f\"Train: {X_train.shape}\")\n",
        "print(f\"Valid: {X_valid.shape}\")\n",
        "print(f\"Test: {X_test.shape}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # !conda install pyarrow -y \n",
        "# case_ids = data[\"case_id\"].unique().shuffle(seed=1)\n",
        "# case_ids_train, case_ids_test = train_test_split(case_ids, train_size=0.6, random_state=1)\n",
        "# case_ids_valid, case_ids_test = train_test_split(case_ids_test, train_size=0.5, random_state=1)\n",
        "\n",
        "# cols_pred = []\n",
        "# for col in data.columns:\n",
        "#     if col[-1].isupper() and col[:-1].islower():\n",
        "#         cols_pred.append(col)\n",
        "\n",
        "# print(cols_pred)\n",
        "\n",
        "# def from_polars_to_pandas(case_ids: pl.DataFrame) -> pl.DataFrame:\n",
        "#     return (\n",
        "#         data.filter(pl.col(\"case_id\").is_in(case_ids))[[\"case_id\", \"WEEK_NUM\", \"target\"]].to_pandas(),\n",
        "#         data.filter(pl.col(\"case_id\").is_in(case_ids))[cols_pred].to_pandas(),\n",
        "#         data.filter(pl.col(\"case_id\").is_in(case_ids))[\"target\"].to_pandas()\n",
        "#     )\n",
        "\n",
        "# base_train, X_train, y_train = from_polars_to_pandas(case_ids_train)\n",
        "# base_valid, X_valid, y_valid = from_polars_to_pandas(case_ids_valid)\n",
        "# base_test, X_test, y_test = from_polars_to_pandas(case_ids_test)\n",
        "\n",
        "# for df in [X_train, X_valid, X_test]:\n",
        "#     df = convert_strings(df)\n",
        "\n",
        "# !conda install pyarrow -y \n",
        "\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch \n",
        "import polars as pl\n",
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "\n",
        "\n",
        "case_ids = data[\"case_id\"].unique().shuffle(seed=1)\n",
        "case_ids_train, case_ids_test = train_test_split(case_ids, train_size=0.6, random_state=1)\n",
        "case_ids_valid, case_ids_test = train_test_split(case_ids_test, train_size=0.5, random_state=1)\n",
        "\n",
        "cols_pred = []\n",
        "for col in data.columns:\n",
        "    if (col[-1].isupper() and col[:-1].islower()) or col == 'date_decision' or col == 'totaldebt_9A':\n",
        "        cols_pred.append(col)\n",
        "\n",
        "# print(cols_pred)\n",
        "\n",
        "# Add this line to print all column names from the dataset\n",
        "print(\"All column names in the dataset:\", cols_pred)\n",
        "\n",
        "def from_polars_to_pandas(case_ids: pl.DataFrame) -> pl.DataFrame:\n",
        "    return (\n",
        "        data.filter(pl.col(\"case_id\").is_in(case_ids))[[\"case_id\", \"WEEK_NUM\", \"target\"]].to_pandas(),\n",
        "        data.filter(pl.col(\"case_id\").is_in(case_ids))[cols_pred].to_pandas(),\n",
        "        data.filter(pl.col(\"case_id\").is_in(case_ids))[\"target\"].to_pandas()\n",
        "    )\n",
        "\n",
        "# base_train, X_train, y_train = from_polars_to_pandas(case_ids_train)\n",
        "# base_valid, X_valid, y_valid = from_polars_to_pandas(case_ids_valid)\n",
        "# base_test, X_test, y_test = from_polars_to_pandas(case_ids_test)\n",
        "\n",
        "mx_smpls = 900000\n",
        "# Convert from Polars to Pandas and then take only the first 100 samples for training\n",
        "base_train, X_train, y_train = from_polars_to_pandas(case_ids_train)\n",
        "print(base_train.shape)\n",
        "base_train, X_train, y_train = base_train.head(mx_smpls), X_train.head(mx_smpls), y_train.head(mx_smpls)\n",
        "\n",
        "# Convert from Polars to Pandas and then take only the first 100 samples for validation\n",
        "base_valid, X_valid, y_valid = from_polars_to_pandas(case_ids_valid)\n",
        "base_valid, X_valid, y_valid = base_valid.head(mx_smpls), X_valid.head(mx_smpls), y_valid.head(mx_smpls)\n",
        "\n",
        "# Convert from Polars to Pandas and then take only the first 100 samples for testing\n",
        "base_test, X_test, y_test = from_polars_to_pandas(case_ids_test)\n",
        "base_test, X_test, y_test = base_test.head(mx_smpls), X_test.head(mx_smpls), y_test.head(mx_smpls)\n",
        "\n",
        "\n",
        "for df in [X_train, X_valid, X_test]:\n",
        "    df = convert_strings(df)\n",
        "\n",
        "\n",
        "covid_start_date = pd.to_datetime('2020-01-01')\n",
        "\n",
        "def convert_date_to_binary(df):\n",
        "    # Ensure date_decision is in datetime format\n",
        "    df['date_decision'] = pd.to_datetime(df['date_decision'])\n",
        "    # Convert date_decision to binary: 1 if during or after COVID start, 0 otherwise\n",
        "    df['date_decision'] = (df['date_decision'] >= covid_start_date).astype(int)\n",
        "    return df\n",
        "\n",
        "# Now apply this function to your DataFrames\n",
        "X_train = convert_date_to_binary(X_train)\n",
        "X_valid = convert_date_to_binary(X_valid)\n",
        "X_test = convert_date_to_binary(X_test)\n",
        "\n",
        "    \n",
        "print(\"Sample values from X_train:\")\n",
        "for col in X_train.columns:\n",
        "    print(f\"{col}: {X_train[col].sample(10).to_list()}\")\n",
        "\n",
        "##### cutoff \n",
        "\n",
        "# Function to manually create one-hot encoding for numeric data\n",
        "def one_hot_numeric(series, num_bins=100):\n",
        "    clean_series = series.dropna()\n",
        "    mu, sigma = clean_series.mean(), clean_series.std()\n",
        "    percentiles = series.apply(lambda x: norm.cdf(x, mu, sigma) if pd.notnull(x) else np.nan)\n",
        "    bins = np.linspace(0, 1, num_bins + 1)\n",
        "    one_hot_matrix = np.zeros((len(series), num_bins))\n",
        "    for i, val in enumerate(percentiles):\n",
        "        if pd.notnull(val):\n",
        "            bin_index = np.digitize(val, bins) - 1\n",
        "            one_hot_matrix[i, min(bin_index, num_bins - 1)] = 1\n",
        "        else:\n",
        "            one_hot_matrix[i, -1] = 1\n",
        "    return one_hot_matrix\n",
        "\n",
        "# Function to process categorical data\n",
        "def process_categorical(series):\n",
        "    return pd.get_dummies(series, dummy_na=True, prefix='category')\n",
        "\n",
        "# Process each column in data_df\n",
        "\n",
        "#### memory error for the below \n",
        "\n",
        "# def preprocess(data_df):\n",
        "#     one_hot_encoded_tensors = []\n",
        "#     for column in data_df:\n",
        "#         series = data_df[column]\n",
        "#         if series.dtype == 'float64':\n",
        "#             one_hot_matrix = one_hot_numeric(series)\n",
        "#             one_hot_encoded_tensors.append(torch.tensor(one_hot_matrix, dtype=torch.float32))\n",
        "#         elif series.dtype == 'object':\n",
        "#             one_hot_df = process_categorical(series)\n",
        "#             one_hot_encoded_tensors.append(torch.tensor(one_hot_df.values, dtype=torch.float32))\n",
        "\n",
        "#     def convert_one_hot_to_tokens(one_hot_tensors):\n",
        "#         token_sequences = []\n",
        "#         offset = 0\n",
        "#         for tensor in one_hot_tensors:\n",
        "#             tokens = [(torch.argmax(vector).item() + offset) if vector.sum() != 0 else -1 for vector in tensor]\n",
        "#             token_sequences.append(tokens)\n",
        "#             offset += tensor.shape[1]\n",
        "#         return token_sequences\n",
        "\n",
        "#     def combine_tokens(token_sequences):\n",
        "#         combined_sequences = []\n",
        "#         for sequence in zip(*token_sequences):\n",
        "#             combined_sequences.append(list(sequence))\n",
        "#         return combined_sequences\n",
        "\n",
        "#     token_sequences = convert_one_hot_to_tokens(one_hot_encoded_tensors)\n",
        "#     combined_sequences = combine_tokens(token_sequences)\n",
        "#     return combined_sequences\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Function to manually create one-hot encoding for numeric data\n",
        "def one_hot_numeric(series, num_bins=10):\n",
        "    clean_series = series.dropna()\n",
        "    mu, sigma = clean_series.mean(), clean_series.std()\n",
        "    percentiles = series.apply(lambda x: norm.cdf(x, mu, sigma) if pd.notnull(x) else np.nan)\n",
        "    bins = np.linspace(0, 1, num_bins + 1)\n",
        "    one_hot_matrix = np.zeros((len(series), num_bins))\n",
        "    for i, val in enumerate(percentiles):\n",
        "        if pd.notnull(val):\n",
        "            bin_index = np.digitize(val, bins) - 1\n",
        "            one_hot_matrix[i, min(bin_index, num_bins - 1)] = 1\n",
        "        else:\n",
        "            one_hot_matrix[i, -1] = 1\n",
        "    return one_hot_matrix\n",
        "\n",
        "# Function to process categorical data\n",
        "def process_categorical(series):\n",
        "    # Instead of using pd.get_dummies, use OneHotEncoder for more efficiency\n",
        "    encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
        "    one_hot_array = encoder.fit_transform(series.values.reshape(-1, 1))\n",
        "    return one_hot_array\n",
        "\n",
        "# Process each column in data_df\n",
        "def preprocess(data_df):\n",
        "    one_hot_encoded_arrays = []\n",
        "    for column in data_df:\n",
        "        series = data_df[column]\n",
        "        if series.dtype == 'float64':\n",
        "            one_hot_matrix = one_hot_numeric(series)\n",
        "            one_hot_encoded_arrays.append(one_hot_matrix)\n",
        "        elif series.dtype == 'object':\n",
        "            one_hot_array = process_categorical(series)\n",
        "            one_hot_encoded_arrays.append(one_hot_array)\n",
        "\n",
        "    # Combine the one-hot encoded arrays\n",
        "    combined_sequences = np.hstack(one_hot_encoded_arrays)\n",
        "    return combined_sequences\n",
        "\n",
        "\n",
        "# tr_pre = X_train\n",
        "# val_pre = X_valid\n",
        "# test_pre = X_test\n",
        "\n",
        "# print(tr_pre.shape, val_pre.shape)  \n",
        "\n",
        "\n",
        "combined_sequences = preprocess(data_df=X_train)\n",
        "combined_sequences_val = preprocess(data_df=X_train)\n",
        "\n",
        "# print(\"Combined token sequences for input to transformer:\")\n",
        "# for seq in combined_sequences:\n",
        "#     print(seq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# from torch.optim import Adam\n",
        "# import torch\n",
        "\n",
        "# # Clear cache memory\n",
        "# torch.cuda.empty_cache()\n",
        "\n",
        "# # Reset peak memory statistics\n",
        "# torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "# # If you want to clear the memory for a specific device, you can specify the device index\n",
        "# device_index = 0  # For example, for the first CUDA device\n",
        "# torch.cuda.empty_cache(device_index)\n",
        "# torch.cuda.reset_peak_memory_stats(device_index)\n",
        "\n",
        "# import pickle\n",
        "\n",
        "# # Assuming combined_sequences and combined_sequences_val are lists or similar structures\n",
        "# def save_data_to_pickle(data, filename):\n",
        "#     with open(filename, 'wb') as file:\n",
        "#         pickle.dump(data, file)\n",
        "\n",
        "\n",
        "# save_data_to_pickle(combined_sequences, 'combined_sequences.pkl')\n",
        "\n",
        "# # Save combined_sequences_val\n",
        "# save_data_to_pickle(combined_sequences_val, 'combined_sequences_val.pkl')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OAyZN5rKtuo"
      },
      "source": [
        "# Training LightGBM\n",
        "\n",
        "Minimal example of LightGBM training is shown below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0/4999\n",
            "Training Accuracy: 50.397%\n",
            "Validation Accuracy: 96.963%\n",
            "Validation AUC: 0.6184197397314017\n",
            "Epoch 1/4999\n",
            "Training Accuracy: 64.928%\n",
            "Validation Accuracy: 52.292%\n",
            "Validation AUC: 0.6636793562133884\n",
            "Epoch 2/4999\n",
            "Training Accuracy: 68.023%\n",
            "Validation Accuracy: 47.567%\n",
            "Validation AUC: 0.6659052110279038\n",
            "Epoch 3/4999\n",
            "Training Accuracy: 68.604%\n",
            "Validation Accuracy: 67.247%\n",
            "Validation AUC: 0.6744656752910184\n",
            "Epoch 4/4999\n",
            "Training Accuracy: 69.073%\n",
            "Validation Accuracy: 78.215%\n",
            "Validation AUC: 0.6641615582066046\n",
            "Epoch 5/4999\n",
            "Training Accuracy: 68.586%\n",
            "Validation Accuracy: 72.729%\n",
            "Validation AUC: 0.66708478360396\n",
            "Epoch 6/4999\n",
            "Training Accuracy: 69.196%\n",
            "Validation Accuracy: 78.851%\n",
            "Validation AUC: 0.6387704317224152\n",
            "Epoch 7/4999\n",
            "Training Accuracy: 69.258%\n",
            "Validation Accuracy: 82.985%\n",
            "Validation AUC: 0.6640425676088684\n",
            "Epoch 8/4999\n",
            "Training Accuracy: 69.552%\n",
            "Validation Accuracy: 83.083%\n",
            "Validation AUC: 0.6588633115027827\n",
            "Epoch 9/4999\n",
            "Training Accuracy: 69.647%\n",
            "Validation Accuracy: 76.875%\n",
            "Validation AUC: 0.6437158968393261\n",
            "Epoch 10/4999\n",
            "Training Accuracy: 69.204%\n",
            "Validation Accuracy: 76.281%\n",
            "Validation AUC: 0.6643850750889684\n",
            "Epoch 11/4999\n",
            "Training Accuracy: 69.372%\n",
            "Validation Accuracy: 64.234%\n",
            "Validation AUC: 0.655034441483384\n",
            "Epoch 12/4999\n",
            "Training Accuracy: 66.655%\n",
            "Validation Accuracy: 61.575%\n",
            "Validation AUC: 0.6054137203036117\n",
            "Epoch 13/4999\n",
            "Training Accuracy: 57.166%\n",
            "Validation Accuracy: 68.001%\n",
            "Validation AUC: 0.586725680268488\n",
            "Epoch 14/4999\n",
            "Training Accuracy: 61.496%\n",
            "Validation Accuracy: 79.199%\n",
            "Validation AUC: 0.6305877957128084\n",
            "Epoch 15/4999\n",
            "Training Accuracy: 61.606%\n",
            "Validation Accuracy: 96.546%\n",
            "Validation AUC: 0.6225422109119394\n",
            "Epoch 16/4999\n",
            "Training Accuracy: 62.323%\n",
            "Validation Accuracy: 70.003%\n",
            "Validation AUC: 0.6054231793006419\n",
            "Epoch 17/4999\n",
            "Training Accuracy: 64.139%\n",
            "Validation Accuracy: 75.563%\n",
            "Validation AUC: 0.6400483682971994\n",
            "Epoch 18/4999\n",
            "Training Accuracy: 62.591%\n",
            "Validation Accuracy: 60.246%\n",
            "Validation AUC: 0.6269570473975701\n",
            "Epoch 19/4999\n",
            "Training Accuracy: 55.081%\n",
            "Validation Accuracy: 3.002%\n",
            "Validation AUC: 0.6058708512146221\n",
            "Epoch 20/4999\n",
            "Training Accuracy: 59.832%\n",
            "Validation Accuracy: 3.579%\n",
            "Validation AUC: 0.603970368314608\n",
            "Epoch 21/4999\n",
            "Training Accuracy: 61.319%\n",
            "Validation Accuracy: 2.884%\n",
            "Validation AUC: 0.6124777525685497\n",
            "Epoch 22/4999\n",
            "Training Accuracy: 62.542%\n",
            "Validation Accuracy: 4.782%\n",
            "Validation AUC: 0.6196240151894338\n",
            "Epoch 23/4999\n",
            "Training Accuracy: 62.188%\n",
            "Validation Accuracy: 57.707%\n",
            "Validation AUC: 0.5948340576854042\n",
            "Epoch 24/4999\n",
            "Training Accuracy: 64.202%\n",
            "Validation Accuracy: 2.916%\n",
            "Validation AUC: 0.6167208843738246\n",
            "Epoch 25/4999\n",
            "Training Accuracy: 64.866%\n",
            "Validation Accuracy: 2.954%\n",
            "Validation AUC: 0.6230138397555458\n",
            "Epoch 26/4999\n",
            "Training Accuracy: 65.921%\n",
            "Validation Accuracy: 2.955%\n",
            "Validation AUC: 0.6238627196237763\n",
            "Epoch 27/4999\n",
            "Training Accuracy: 66.767%\n",
            "Validation Accuracy: 3.445%\n",
            "Validation AUC: 0.6234737552555532\n",
            "Epoch 28/4999\n",
            "Training Accuracy: 66.989%\n",
            "Validation Accuracy: 3.1%\n",
            "Validation AUC: 0.6349805691765917\n",
            "Epoch 29/4999\n",
            "Training Accuracy: 67.2%\n",
            "Validation Accuracy: 3.128%\n",
            "Validation AUC: 0.6364713803464912\n",
            "Epoch 30/4999\n",
            "Training Accuracy: 67.243%\n",
            "Validation Accuracy: 2.935%\n",
            "Validation AUC: 0.621300368903565\n",
            "Epoch 31/4999\n",
            "Training Accuracy: 66.567%\n",
            "Validation Accuracy: 6.747%\n",
            "Validation AUC: 0.6454194134366136\n",
            "Epoch 32/4999\n",
            "Training Accuracy: 67.354%\n",
            "Validation Accuracy: 22.126%\n",
            "Validation AUC: 0.6399184020258091\n",
            "Epoch 33/4999\n",
            "Training Accuracy: 67.48%\n",
            "Validation Accuracy: 4.998%\n",
            "Validation AUC: 0.6178888647331939\n",
            "Epoch 34/4999\n",
            "Training Accuracy: 67.797%\n",
            "Validation Accuracy: 9.012%\n",
            "Validation AUC: 0.6426230772525156\n",
            "Epoch 35/4999\n",
            "Training Accuracy: 67.741%\n",
            "Validation Accuracy: 11.74%\n",
            "Validation AUC: 0.6380394255851378\n",
            "Epoch 36/4999\n",
            "Training Accuracy: 68.261%\n",
            "Validation Accuracy: 4.812%\n",
            "Validation AUC: 0.6327213579901119\n",
            "Epoch 37/4999\n",
            "Training Accuracy: 69.293%\n",
            "Validation Accuracy: 3.337%\n",
            "Validation AUC: 0.6274897816592703\n",
            "Epoch 38/4999\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[25], line 347\u001b[0m\n\u001b[1;32m    345\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(batch_sequences)\n\u001b[1;32m    346\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(outputs, batch_labels)\n\u001b[0;32m--> 347\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# Calculate training accuracy\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import Adam\n",
        "import torch\n",
        "\n",
        "import pickle\n",
        "\n",
        "def load_data_from_pickle(filename):\n",
        "    with open(filename, 'rb') as file:\n",
        "        data = pickle.load(file)\n",
        "    return data\n",
        "\n",
        "# Load combined_sequences and combined_sequences_val\n",
        "combined_sequences = load_data_from_pickle('combined_sequences_lg.pkl')\n",
        "combined_sequences_val = load_data_from_pickle('combined_sequences_val_lg.pkl')\n",
        "\n",
        "\n",
        "# import torch\n",
        "import math\n",
        "\n",
        "\n",
        "\n",
        "# print(f\"Train: {X_train.shape}\")\n",
        "# print(f\"Valid: {X_valid.shape}\")\n",
        "# print(f\"Test: {X_test.shape}\")\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from torch import nn, optim\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "# class SimpleTransformer(nn.Module):\n",
        "#     def __init__(self, input_dim, model_dim, num_classes, num_heads=2, num_layers=1, dropout=0.1, mlp_layers=3):\n",
        "#         super(SimpleTransformer, self).__init__()\n",
        "#         self.embed = nn.Embedding(input_dim, model_dim)\n",
        "#         encoder_layer = nn.TransformerEncoderLayer(d_model=model_dim, nhead=num_heads, dropout=dropout)\n",
        "#         self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        \n",
        "#         # MLP layers\n",
        "#         mlp_modules = [nn.Linear(model_dim, model_dim * 2), nn.ReLU()]\n",
        "#         for _ in range(mlp_layers - 1):\n",
        "#             mlp_modules.extend([nn.Linear(model_dim * 2, model_dim * 2), nn.ReLU()])\n",
        "#         mlp_modules.append(nn.Linear(model_dim * 2, num_classes))\n",
        "#         self.mlp = nn.Sequential(*mlp_modules)\n",
        "        \n",
        "#         # Positional encoding\n",
        "#         self.positional_encoding = self.create_positional_encoding(175, model_dim)  # Keep max_len at 174\n",
        "\n",
        "#     def create_positional_encoding(self, max_len, model_dim):\n",
        "#         pe = torch.zeros(max_len, model_dim)\n",
        "#         position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "#         div_term = torch.exp(torch.arange(0, model_dim, 2).float() * (-math.log(10000.0) / model_dim))\n",
        "#         pe[:, 0::2] = torch.sin(position * div_term)\n",
        "#         pe[:, 1::2] = torch.cos(position * div_term)\n",
        "#         pe = pe.unsqueeze(0)  # Add batch dimension for broadcasting\n",
        "#         return pe.cuda()\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.embed(x)  # Convert token indices to embeddings\n",
        "#         x += self.positional_encoding  # Add positional encoding to embeddings\n",
        "#         x = self.transformer_encoder(x)\n",
        "#         x = torch.mean(x, dim=1)  # Average pooling across the sequence dimension\n",
        "#         x = self.mlp(x)  # Pass through MLP\n",
        "#         return x\n",
        "# # Define a simple Transformer model for demonstration purposes\n",
        "    \n",
        "\n",
        "class SimpleTransformer(nn.Module):\n",
        "    def __init__(self, input_dim, model_dim, num_classes, num_heads=2, num_layers=1, dropout=0.1):\n",
        "        super(SimpleTransformer, self).__init__()\n",
        "        self.embed = nn.Embedding(input_dim, model_dim)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=model_dim, nhead=num_heads, dropout=dropout)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.output_layer = nn.Linear(model_dim, num_classes)\n",
        "        self.positional_encoding = self.create_positional_encoding(174+1, model_dim)  # Create positional encoding\n",
        "\n",
        "    def create_positional_encoding(self, max_len, model_dim):\n",
        "        \"\"\"Create positional encoding matrix.\"\"\"\n",
        "        # Create a matrix that contains the positional encodings for all possible positions\n",
        "        pe = torch.zeros(max_len, model_dim)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, model_dim, 2).float() * (-math.log(10000.0) / model_dim))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)  # Add batch dimension for broadcasting\n",
        "        return pe.cuda()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embed(x)  # Convert token indices to embeddings\n",
        "        x = x + self.positional_encoding# Add positional encoding to embeddings\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = torch.mean(x, dim=1)  # Average pooling\n",
        "        return self.output_layer(x)\n",
        "\n",
        "# class TokenSequenceDataset(Dataset):\n",
        "#     def __init__(self, sequences, labels):\n",
        "#         self.sequences = sequences\n",
        "#         self.labels = labels\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.sequences)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         sequence = torch.tensor(self.sequences[idx], dtype=torch.long)\n",
        "#         label = torch.tensor(self.labels[idx], dtype=torch.long)  # Ensure labels are long type for classification\n",
        "#         return sequence, label\n",
        "\n",
        "import pickle\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Function to load predictions from pickle files\n",
        "def load_predictions(file_path):\n",
        "    with open(file_path, 'rb') as file:\n",
        "        return pickle.load(file)\n",
        "\n",
        "# Function to convert prediction scores into integer tokens\n",
        "def convert_scores_to_tokens(scores, num_bins=100):\n",
        "    # Assuming scores are in the range [0, 1], scale them to [1, 100]\n",
        "    return [int(score * (num_bins - 1)) + 10000 for score in scores]\n",
        "\n",
        "# Your modified TokenSequenceDataset class\n",
        "class TokenSequenceDataset(Dataset):\n",
        "    def __init__(self, sequences, labels, prediction_scores):\n",
        "        self.sequences = sequences\n",
        "        self.labels = labels\n",
        "        self.prediction_tokens = convert_scores_to_tokens(prediction_scores)\n",
        "        assert len(self.sequences) == len(self.prediction_tokens), \"Sequences and predictions must have the same length\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Add the prediction token to the end of the sequence\n",
        "        sequence_with_token = self.sequences[idx] + [self.prediction_tokens[idx]]\n",
        "        sequence_tensor = torch.tensor(sequence_with_token, dtype=torch.long)\n",
        "        label_tensor = torch.tensor(self.labels[idx], dtype=torch.long)  # Ensure labels are long type for classification\n",
        "        return sequence_tensor, label_tensor\n",
        "\n",
        "# Example usage\n",
        "train_pred_scores = load_predictions('/home/brett/Desktop/tutorials/kag/y_pred_train.pkl')\n",
        "valid_pred_scores = load_predictions('/home/brett/Desktop/tutorials/kag/y_pred_valid.pkl')\n",
        "\n",
        "\n",
        "\n",
        "# # Create the dataset\n",
        "# train_dataset = TokenSequenceDataset(train_sequences, train_labels, train_pred_scores)\n",
        "\n",
        "# # Example of using DataLoader\n",
        "# train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "train_dataset = TokenSequenceDataset(combined_sequences, y_train.to_numpy(), train_pred_scores)\n",
        "\n",
        "val_dataset = TokenSequenceDataset(combined_sequences_val, y_valid.to_numpy(), valid_pred_scores)\n",
        "\n",
        "# class TokenSequenceDataset(Dataset):\n",
        "#     def __init__(self, sequences, labels):\n",
        "#         self.sequences = sequences\n",
        "#         self.labels = labels\n",
        "#         # Define tokens for negative and positive classes\n",
        "#         self.neg_token = 0000\n",
        "#         self.pos_token = 1111\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.sequences)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         # Copy the original sequence to avoid modifying the original data\n",
        "#         sequence = list(self.sequences[idx])  # Convert to list if it's not already\n",
        "#         label = self.labels[idx]\n",
        "        \n",
        "#         # Append special token based on the label\n",
        "#         if label == 0:  # Assuming 0 is your negative class\n",
        "#             sequence.append(self.neg_token)\n",
        "#         else:  # Assuming 1 or any non-zero value is your positive class\n",
        "#             sequence.append(self.pos_token)\n",
        "        \n",
        "#         # Convert to tensors\n",
        "#         sequence_tensor = torch.tensor(sequence, dtype=torch.long)\n",
        "#         label_tensor = torch.tensor(label, dtype=torch.long)  # Ensure labels are long type for classification\n",
        "        \n",
        "#         return sequence_tensor, label_tensor\n",
        "\n",
        "# Model parameters\n",
        "INPUT_DIM = max(max(seq) for seq in combined_sequences) + 1  # Plus one since we're using these as embeddings indices\n",
        "MODEL_DIM = 128\n",
        "NUM_CLASSES = 2  # Adjust based on your actual number of classes\n",
        "NUM_HEADS = 16\n",
        "NUM_LAYERS = 2\n",
        "DROPOUT = 0.1\n",
        "\n",
        "# # Initialize model, loss function, and optimizer\n",
        "# model = SimpleTransformer(INPUT_DIM, MODEL_DIM, NUM_CLASSES, NUM_HEADS, NUM_LAYERS, DROPOUT).cuda\n",
        "# loss_function = nn.CrossEntropyLoss()\n",
        "# optimizer = Adam(model.parameters())\n",
        "\n",
        "# train_dataset = TokenSequenceDataset(combined_sequences, y_train.to_numpy())  # Assuming y_train is your training labels\n",
        "# val_dataset = TokenSequenceDataset(combined_sequences_val, y_valid.to_numpy())  # And y_valid for validation\n",
        "\n",
        "# # Create DataLoaders\n",
        "# train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "# val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False)  # Typically no shuffle for validation data\n",
        "\n",
        "\n",
        "# from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# # Training loop adjustments\n",
        "# num_epochs = 50  # Set this to your desired number of epochs\n",
        "# for epoch in range(num_epochs):\n",
        "#     print(epoch)\n",
        "#     model.train()  # Set the model to training mode\n",
        "#     for batch_sequences, batch_labels in train_dataloader:\n",
        "#         optimizer.zero_grad()\n",
        "#         outputs = model(batch_sequences)\n",
        "#         loss = loss_function(outputs, batch_labels)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "    \n",
        "#     # Print the average loss after each epoch (optional, you could add logic here to track and print this)\n",
        "\n",
        "#     # Validation loop adjustments\n",
        "#     model.eval()  # Set the model to evaluation mode\n",
        "#     all_preds = []\n",
        "#     all_labels = []\n",
        "#     with torch.no_grad():  # No need to track gradients for validation\n",
        "#         for batch_sequences, batch_labels in val_dataloader:\n",
        "#             outputs = model(batch_sequences)\n",
        "#             all_labels.extend(batch_labels.tolist())  # Collecting actual labels\n",
        "\n",
        "#             # Applying softmax to get probabilities, assuming your task is binary classification\n",
        "#             # For multi-class, adjust accordingly\n",
        "#             probas = outputs.softmax(dim=1)[:, 1]  # get the probability of the positive class\n",
        "#             all_preds.extend(probas.tolist())\n",
        "\n",
        "#     # Calculate AUC\n",
        "#     # Ensure all_labels and all_preds are both populated with all data from validation set\n",
        "#     auc_score = roc_auc_score(all_labels, all_preds)\n",
        "#     print(f\"Epoch {epoch}, Validation AUC: {auc_score}\")\n",
        "\n",
        "# # Training loop\n",
        "# # Training loop\n",
        "# for epoch in range(1):  # Increase number of epochs as needed\n",
        "#     for i, (batch_sequences, batch_labels) in enumerate(dataloader):\n",
        "#         optimizer.zero_grad()\n",
        "#         outputs = model(batch_sequences)\n",
        "#         loss = loss_function(outputs, batch_labels)  # Use the actual labels\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         print(f\"Epoch {epoch}, Batch {i}, Loss: {loss.item()}\")\n",
        "\n",
        "\n",
        "\n",
        "# Initialize the MinMaxScaler\n",
        "# scaler = MinMaxScaler()\n",
        "\n",
        "# # Fit the scaler using only the training data and transform training data\n",
        "# X_train_scaled = scaler.fit_transform(X_train.select_dtypes(include=[np.number]))  # Apply only to numeric columns\n",
        "# X_train = pd.DataFrame(X_train_scaled, columns=X_train.select_dtypes(include=[np.number]).columns)\n",
        "\n",
        "# # Apply the same transformation to validation and test data\n",
        "# X_valid_scaled = scaler.transform(X_valid.select_dtypes(include=[np.number]))\n",
        "# X_valid = pd.DataFrame(X_valid_scaled, columns=X_valid.select_dtypes(include=[np.number]).columns)\n",
        "\n",
        "# X_test_scaled = scaler.transform(X_test.select_dtypes(include=[np.number]))\n",
        "# X_test = pd.DataFrame(X_test_scaled, columns=X_test.select_dtypes(include=[np.number]).columns)\n",
        "\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "model = SimpleTransformer(INPUT_DIM, MODEL_DIM, NUM_CLASSES, NUM_HEADS, NUM_LAYERS, DROPOUT).to(device)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# Assuming y_train and y_valid are your training and validation labels respectively\n",
        "# train_dataset = TokenSequenceDataset(combined_sequences, y_train.to_numpy())\n",
        "# val_dataset = TokenSequenceDataset(combined_sequences_val, y_valid.to_numpy())\n",
        "\n",
        "\n",
        "            \n",
        "from torch.utils.data import WeightedRandomSampler\n",
        "\n",
        "# Calculate class weights inversely proportional to their frequencies\n",
        "class_sample_counts = [np.sum(y_train == t) for t in np.unique(y_train)]\n",
        "class_weights = 1. / torch.tensor(class_sample_counts, dtype=torch.float)\n",
        "# Assign a weight to every data point based on its class\n",
        "sample_weights = class_weights[y_train]\n",
        "\n",
        "# Create a WeightedRandomSampler object\n",
        "sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=256, sampler=sampler)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=256,  sampler=sampler)\n",
        "\n",
        "# Training loop\n",
        "# num_epochs = 50\n",
        "# for epoch in range(num_epochs):\n",
        "#     print(f'Epoch {epoch}/{num_epochs - 1}')\n",
        "#     model.train()  # Set the model to training mode\n",
        "#     for batch_sequences, batch_labels in train_dataloader:\n",
        "#         batch_sequences, batch_labels = batch_sequences.to(device), batch_labels.to(device)\n",
        "#         optimizer.zero_grad()\n",
        "#         outputs = model(batch_sequences)\n",
        "#         loss = loss_function(outputs, batch_labels)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "    \n",
        "#     # Validation loop\n",
        "#     model.eval()  # Set the model to evaluation mode\n",
        "#     all_preds = []\n",
        "#     all_labels = []\n",
        "#     with torch.no_grad():  # No need to track gradients for validation\n",
        "#         for batch_sequences, batch_labels in val_dataloader:\n",
        "#             batch_sequences, batch_labels = batch_sequences.to(device), batch_labels.to(device)\n",
        "#             outputs = model(batch_sequences)\n",
        "#             all_labels.extend(batch_labels.tolist())  # Collecting actual labels\n",
        "\n",
        "#             # Applying softmax to get probabilities, assuming your task is binary classification\n",
        "#             probas = outputs.softmax(dim=1)[:, 1]  # get the probability of the positive class\n",
        "#             all_preds.extend(probas.tolist())\n",
        "\n",
        "#     # Calculate AUC\n",
        "#     auc_score = roc_auc_score(all_labels, all_preds)\n",
        "#     print(f\"Validation AUC: {auc_score}\")\n",
        "        \n",
        "    \n",
        "    \n",
        "num_epochs = 5000\n",
        "for epoch in range(num_epochs):\n",
        "    print(f'Epoch {epoch}/{num_epochs - 1}')\n",
        "    model.train()  # Set the model to training mode\n",
        "    total_train, correct_train = 0, 0\n",
        "    for batch_sequences, batch_labels in train_dataloader:\n",
        "        batch_sequences, batch_labels = batch_sequences.to(device), batch_labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_sequences)\n",
        "        loss = loss_function(outputs, batch_labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Calculate training accuracy\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_train += batch_labels.size(0)\n",
        "        correct_train += (predicted == batch_labels).sum().item()\n",
        "    \n",
        "    train_accuracy = 100 * correct_train / total_train\n",
        "    print(f'Training Accuracy: {train_accuracy}%')\n",
        "    \n",
        "    # Validation loop\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    total_val, correct_val = 0, 0\n",
        "    with torch.no_grad():  # No need to track gradients for validation\n",
        "        for batch_sequences, batch_labels in val_dataloader:\n",
        "            batch_sequences, batch_labels = batch_sequences.to(device), batch_labels.to(device)\n",
        "            outputs = model(batch_sequences)\n",
        "            all_labels.extend(batch_labels.tolist())  # Collecting actual labels\n",
        "\n",
        "            # Applying softmax to get probabilities, assuming your task is binary classification\n",
        "            probas = outputs.softmax(dim=1)[:, 1]  # get the probability of the positive class\n",
        "            all_preds.extend(probas.tolist())\n",
        "\n",
        "            # Calculate validation accuracy\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_val += batch_labels.size(0)\n",
        "            correct_val += (predicted == batch_labels).sum().item()\n",
        "\n",
        "    val_accuracy = 100 * correct_val / total_val\n",
        "    print(f'Validation Accuracy: {val_accuracy}%')\n",
        "\n",
        "    # Calculate AUC\n",
        "    auc_score = roc_auc_score(all_labels, all_preds)\n",
        "    print(f\"Validation AUC: {auc_score}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# del model \n",
        "# # # Clear cache memory\n",
        "# # torch.cuda.empty_cache()\n",
        "\n",
        "# # # Reset peak memory statistics\n",
        "# # torch.cuda.reset_peak_memory_stats()\n",
        "# import torch\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# import torch.optim as optim\n",
        "# from transformers import BertTokenizer, BertForSequenceClassification\n",
        "# from transformers import get_linear_schedule_with_warmup\n",
        "# from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "# import numpy as np\n",
        "\n",
        "# # Load the tokenizer and model from pre-trained BERT\n",
        "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "# model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2).to(device)\n",
        "\n",
        "# class BertTokenSequenceDataset(Dataset):\n",
        "#     def __init__(self, sequences, labels, tokenizer, max_length=512):\n",
        "#         self.tokenizer = tokenizer\n",
        "#         self.texts = sequences\n",
        "#         self.labels = labels\n",
        "#         self.max_length = max_length\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.texts)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         text = self.texts[idx]\n",
        "#         inputs = self.tokenizer.encode_plus(\n",
        "#             text,\n",
        "#             None,\n",
        "#             add_special_tokens=True,\n",
        "#             max_length=self.max_length,\n",
        "#             padding='max_length',\n",
        "#             return_token_type_ids=True,\n",
        "#             truncation=True\n",
        "#         )\n",
        "#         ids = inputs['input_ids']\n",
        "#         mask = inputs['attention_mask']\n",
        "#         return {\n",
        "#             'ids': torch.tensor(ids, dtype=torch.long),\n",
        "#             'mask': torch.tensor(mask, dtype=torch.long),\n",
        "#             'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "#         }\n",
        "\n",
        "# # Assuming you have your sequences and labels prepared\n",
        "# train_dataset = BertTokenSequenceDataset(combined_sequences, y_train, tokenizer)\n",
        "# val_dataset = BertTokenSequenceDataset(combined_sequences_val, y_valid, tokenizer)\n",
        "\n",
        "# train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "# val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# loss_function = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
        "# total_steps = len(train_dataloader) * num_epochs\n",
        "# scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "# num_epochs = 3000  # You can adjust this\n",
        "# for epoch in range(num_epochs):\n",
        "#     model.train()\n",
        "#     total_train_loss = 0\n",
        "#     for batch in train_dataloader:\n",
        "#         batch_sequences = batch['ids'].to(device)\n",
        "#         batch_mask = batch['mask'].to(device)\n",
        "#         batch_labels = batch['labels'].to(device)\n",
        "\n",
        "#         model.zero_grad()\n",
        "#         outputs = model(batch_sequences, token_type_ids=None, attention_mask=batch_mask, labels=batch_labels)\n",
        "#         loss = outputs.loss\n",
        "#         loss.backward()\n",
        "#         total_train_loss += loss.item()\n",
        "#         optimizer.step()\n",
        "#         scheduler.step()\n",
        "\n",
        "#     avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "#     print(f'Epoch {epoch}/{num_epochs - 1}, Training Loss: {avg_train_loss}')\n",
        "\n",
        "#     # Validation loop\n",
        "#     model.eval()\n",
        "#     total_eval_accuracy = 0\n",
        "#     total_eval_loss = 0\n",
        "#     for batch in val_dataloader:\n",
        "#         batch_sequences = batch['ids'].to(device)\n",
        "#         batch_mask = batch['mask'].to(device)\n",
        "#         batch_labels = batch['labels'].to(device)\n",
        "\n",
        "#         with torch.no_grad():\n",
        "#             outputs = model(batch_sequences, token_type_ids=None, attention_mask=batch_mask, labels=batch_labels)\n",
        "            \n",
        "#         loss = outputs.loss\n",
        "#         total_eval_loss += loss.item()\n",
        "#         logits = outputs.logits.detach().cpu().numpy()\n",
        "#         label_ids = batch_labels.to('cpu').numpy()\n",
        "#         predictions = np.argmax(logits, axis=1)\n",
        "#         total_eval_accuracy += accuracy_score(label_ids, predictions)\n",
        "\n",
        "#     avg_val_accuracy = total_eval_accuracy / len(val_dataloader)\n",
        "#     avg_val_loss = total_eval_loss / len(val_dataloader)\n",
        "#     print(f'Epoch {epoch}/{num_epochs - 1}, Validation Accuracy: {avg_val_accuracy}, Validation Loss: {avg_val_loss}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[25], line 101\u001b[0m\n\u001b[1;32m     99\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(batch_sequences, token_type_ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, attention_mask\u001b[38;5;241m=\u001b[39mbatch_mask, labels\u001b[38;5;241m=\u001b[39mbatch_labels)\n\u001b[1;32m    100\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m--> 101\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m total_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    103\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
            "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# # del model \n",
        "# # # Clear cache memory\n",
        "# # torch.cuda.empty_cache()\n",
        "\n",
        "# # # Reset peak memory statistics\n",
        "# # torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# import torch.optim as optim\n",
        "# from transformers import BertTokenizer, BertForSequenceClassification\n",
        "# from transformers import get_linear_schedule_with_warmup\n",
        "# from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "# import numpy as np\n",
        "\n",
        "# # Assuming your device setup (CUDA or CPU)\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# # Load the tokenizer and model from pre-trained BERT\n",
        "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "# model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2).to(device)\n",
        "\n",
        "# class BertTokenSequenceDataset(Dataset):\n",
        "#     def __init__(self, sequences, labels, tokenizer, max_length=512):\n",
        "#         self.tokenizer = tokenizer\n",
        "#         self.texts = sequences\n",
        "#         self.labels = labels\n",
        "#         self.max_length = max_length\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.texts)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         text = self.texts[idx]\n",
        "#         inputs = self.tokenizer.encode_plus(\n",
        "#             text,\n",
        "#             None,\n",
        "#             add_special_tokens=True,\n",
        "#             max_length=self.max_length,\n",
        "#             padding='max_length',\n",
        "#             return_token_type_ids=True,\n",
        "#             truncation=True\n",
        "#         )\n",
        "#         ids = inputs['input_ids']\n",
        "#         mask = inputs['attention_mask']\n",
        "#         return {\n",
        "#             'ids': torch.tensor(ids, dtype=torch.long),\n",
        "#             'mask': torch.tensor(mask, dtype=torch.long),\n",
        "#             'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "#         }\n",
        "\n",
        "\n",
        "\n",
        "# # Model parameters\n",
        "# INPUT_DIM = max(max(seq) for seq in combined_sequences) + 1  # Plus one since we're using these as embeddings indices\n",
        "# MODEL_DIM = 128\n",
        "# NUM_CLASSES = 2  # Adjust based on your actual number of classes\n",
        "# NUM_HEADS = 16\n",
        "# NUM_LAYERS = 2\n",
        "# DROPOUT = 0.1\n",
        "# num_epochs = 30000\n",
        "# # Assuming you have your sequences and labels prepared\n",
        "# # combined_sequences, y_train, combined_sequences_val, y_valid need to be defined previously\n",
        "# train_dataset = BertTokenSequenceDataset(combined_sequences, y_train, tokenizer)\n",
        "# val_dataset = BertTokenSequenceDataset(combined_sequences_val, y_valid, tokenizer)\n",
        "\n",
        "\n",
        "# from torch.utils.data import WeightedRandomSampler\n",
        "\n",
        "# # Calculate class weights inversely proportional to their frequencies\n",
        "# class_sample_counts = [np.sum(y_train == t) for t in np.unique(y_train)]\n",
        "# class_weights = 1. / torch.tensor(class_sample_counts, dtype=torch.float)\n",
        "# # Assign a weight to every data point based on its class\n",
        "# sample_weights = class_weights[y_train]\n",
        "\n",
        "# # Create a WeightedRandomSampler object\n",
        "# sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
        "\n",
        "# train_dataloader = DataLoader(train_dataset, batch_size=12, sampler=sampler)\n",
        "# val_dataloader = DataLoader(val_dataset, batch_size=12, sampler=sampler)\n",
        "\n",
        "# loss_function = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
        "# total_steps = len(train_dataloader) * num_epochs\n",
        "# scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "# num_epochs = 3  # You can adjust this\n",
        "# for epoch in range(num_epochs):\n",
        "#     model.train()\n",
        "#     total_train_loss = 0\n",
        "#     for batch in train_dataloader:\n",
        "#         batch_sequences = batch['ids'].to(device)\n",
        "#         batch_mask = batch['mask'].to(device)\n",
        "#         batch_labels = batch['labels'].to(device)\n",
        "\n",
        "#         model.zero_grad()\n",
        "#         outputs = model(batch_sequences, token_type_ids=None, attention_mask=batch_mask, labels=batch_labels)\n",
        "#         loss = outputs.loss\n",
        "#         loss.backward()\n",
        "#         total_train_loss += loss.item()\n",
        "#         optimizer.step()\n",
        "#         scheduler.step()\n",
        "\n",
        "#     avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "#     print(f'Epoch {epoch}/{num_epochs - 1}, Training Loss: {avg_train_loss}')\n",
        "\n",
        "#     # Validation loop\n",
        "#     model.eval()\n",
        "#     total_eval_accuracy = 0\n",
        "#     total_eval_loss = 0\n",
        "#     all_labels = []\n",
        "#     all_preds = []\n",
        "#     for batch in val_dataloader:\n",
        "#         batch_sequences = batch['ids'].to(device)\n",
        "#         batch_mask = batch['mask'].to(device)\n",
        "#         batch_labels = batch['labels'].to(device)\n",
        "\n",
        "#         with torch.no_grad():\n",
        "#             outputs = model(batch_sequences, token_type_ids=None, attention_mask=batch_mask, labels=batch_labels)\n",
        "\n",
        "#         loss = outputs.loss\n",
        "#         total_eval_loss += loss.item()\n",
        "#         logits = outputs.logits.detach().cpu().numpy()\n",
        "#         label_ids = batch_labels.to('cpu').numpy()\n",
        "#         predictions = np.argmax(logits, axis=1)\n",
        "#         total_eval_accuracy += accuracy_score(label_ids, predictions)\n",
        "\n",
        "#         # Store all labels and predictions to calculate AUC later\n",
        "#         all_labels.extend(label_ids)\n",
        "#         all_preds.extend(logits[:, 1])  # Assuming the positive class is at index 1\n",
        "\n",
        "#     avg_val_accuracy = total_eval_accuracy / len(val_dataloader)\n",
        "#     avg_val_loss = total_eval_loss / len(val_dataloader)\n",
        "#     val_auc = roc_auc_score(all_labels, all_preds)  # Calculate AUC\n",
        "#     print(f'Epoch {epoch}/{num_epochs - 1}, Validation Accuracy: {avg_val_accuracy}, Validation Loss: {avg_val_loss}, Validation AUC: {val_auc}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-23T07:51:48.83571Z",
          "iopub.status.busy": "2024-02-23T07:51:48.835338Z",
          "iopub.status.idle": "2024-02-23T07:55:50.510355Z",
          "shell.execute_reply": "2024-02-23T07:55:50.509034Z",
          "shell.execute_reply.started": "2024-02-23T07:51:48.835677Z"
        },
        "id": "RgRfY2GvKtuo",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 10 rounds\n",
            "[50]\tvalid_0's auc: 0.70956\n",
            "[100]\tvalid_0's auc: 0.717443\n",
            "Early stopping, best iteration is:\n",
            "[101]\tvalid_0's auc: 0.718126\n",
            "Training Accuracy: 0.964\n",
            "Validation Accuracy: 0.970\n"
          ]
        }
      ],
      "source": [
        "lgb_train = lgb.Dataset(X_train, label=y_train)\n",
        "lgb_valid = lgb.Dataset(X_valid, label=y_valid, reference=lgb_train)\n",
        "\n",
        "params = {\n",
        "    \"boosting_type\": \"gbdt\",\n",
        "    \"objective\": \"binary\",\n",
        "    \"metric\": \"auc\",\n",
        "    \"max_depth\": 4,\n",
        "    \"num_leaves\": 31,\n",
        "    \"learning_rate\": 0.05,\n",
        "    \"feature_fraction\": 0.9,\n",
        "    \"bagging_fraction\": 0.8,\n",
        "    \"bagging_freq\": 5,\n",
        "    \"n_estimators\": 1000,\n",
        "    \"verbose\": -1,\n",
        "}\n",
        "\n",
        "gbm = lgb.train(\n",
        "    params,\n",
        "    lgb_train,\n",
        "    valid_sets=lgb_valid,\n",
        "    callbacks=[lgb.log_evaluation(50), lgb.early_stopping(10)]\n",
        ")\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Generate predictions\n",
        "y_pred_train = np.round(gbm.predict(X_train))  # Threshold at 0.5 for binary classification\n",
        "y_pred_valid = np.round(gbm.predict(X_valid))  # Threshold at 0.5 for binary classification\n",
        "\n",
        "# Calculate accuracy\n",
        "train_accuracy = np.mean(y_pred_train == y_train)\n",
        "valid_accuracy = np.mean(y_pred_valid == y_valid)\n",
        "\n",
        "print(f\"Training Accuracy: {train_accuracy:.3f}\")\n",
        "print(f\"Validation Accuracy: {valid_accuracy:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgA8Ru3xKtuo"
      },
      "source": [
        "Evaluation with AUC and then comparison with the stability metric is shown below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-23T07:55:50.512332Z",
          "iopub.status.busy": "2024-02-23T07:55:50.511918Z",
          "iopub.status.idle": "2024-02-23T07:57:25.418639Z",
          "shell.execute_reply": "2024-02-23T07:57:25.417074Z",
          "shell.execute_reply.started": "2024-02-23T07:55:50.512295Z"
        },
        "id": "1Ave_Sz0Ktuo",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 10 rounds\n",
            "[50]\tvalid_0's auc: 0.70956\n",
            "[100]\tvalid_0's auc: 0.717443\n",
            "Early stopping, best iteration is:\n",
            "[101]\tvalid_0's auc: 0.718126\n",
            "Training Accuracy: 0.964\n",
            "Validation Accuracy: 0.970\n"
          ]
        }
      ],
      "source": [
        "# for base, X in [(base_train, X_train), (base_valid, X_valid), (base_test, X_test)]:\n",
        "#     y_pred = gbm.predict(X, num_iteration=gbm.best_iteration)\n",
        "#     base[\"score\"] = y_pred\n",
        "\n",
        "# print(f'The AUC score on the train set is: {roc_auc_score(base_train[\"target\"], base_train[\"score\"])}')\n",
        "# print(f'The AUC score on the valid set is: {roc_auc_score(base_valid[\"target\"], base_valid[\"score\"])}')\n",
        "# print(f'The AUC score on the test set is: {roc_auc_score(base_test[\"target\"], base_test[\"score\"])}')\n",
        "\n",
        "\n",
        "# def gini_stability(base, w_fallingrate=88.0, w_resstd=-0.5):\n",
        "#     gini_in_time = base.loc[:, [\"WEEK_NUM\", \"target\", \"score\"]]\\\n",
        "#         .sort_values(\"WEEK_NUM\")\\\n",
        "#         .groupby(\"WEEK_NUM\")[[\"target\", \"score\"]]\\\n",
        "#         .apply(lambda x: 2*roc_auc_score(x[\"target\"], x[\"score\"])-1).tolist()\n",
        "\n",
        "#     x = np.arange(len(gini_in_time))\n",
        "#     y = gini_in_time\n",
        "#     a, b = np.polyfit(x, y, 1)\n",
        "#     y_hat = a*x + b\n",
        "#     residuals = y - y_hat\n",
        "#     res_std = np.std(residuals)\n",
        "#     avg_gini = np.mean(gini_in_time)\n",
        "#     return avg_gini + w_fallingrate * min(0, a) + w_resstd * res_std\n",
        "\n",
        "# stability_score_train = gini_stability(base_train)\n",
        "# stability_score_valid = gini_stability(base_valid)\n",
        "# stability_score_test = gini_stability(base_test)\n",
        "\n",
        "# print('\\n')\n",
        "# print(f'The stability score on the train set is: {stability_score_train}')\n",
        "# print(f'The stability score on the valid set is: {stability_score_valid}')\n",
        "# print(f'The stability score on the test set is: {stability_score_test}')\n",
        "\n",
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "# Define datasets for LightGBM\n",
        "lgb_train = lgb.Dataset(X_train, label=y_train)\n",
        "lgb_valid = lgb.Dataset(X_valid, label=y_valid, reference=lgb_train)\n",
        "\n",
        "# Set parameters for the model\n",
        "params = {\n",
        "    \"boosting_type\": \"gbdt\",\n",
        "    \"objective\": \"binary\",\n",
        "    \"metric\": \"auc\",\n",
        "    \"max_depth\": 4,\n",
        "    \"num_leaves\": 31,\n",
        "    \"learning_rate\": 0.05,\n",
        "    \"feature_fraction\": 0.9,\n",
        "    \"bagging_fraction\": 0.8,\n",
        "    \"bagging_freq\": 5,\n",
        "    \"n_estimators\": 1000,\n",
        "    \"verbose\": -1,\n",
        "}\n",
        "\n",
        "# Train the model\n",
        "gbm = lgb.train(\n",
        "    params,\n",
        "    lgb_train,\n",
        "    valid_sets=[lgb_valid],\n",
        "    callbacks=[lgb.log_evaluation(50), lgb.early_stopping(10)]\n",
        ")\n",
        "\n",
        "# Generate raw predictions (these are probabilities, not binary labels)\n",
        "y_pred_train = gbm.predict(X_train)  # Raw probabilities\n",
        "y_pred_valid = gbm.predict(X_valid)  # Raw probabilities\n",
        "\n",
        "# Save the raw predictions to pickle files\n",
        "with open('y_pred_train.pkl', 'wb') as f:\n",
        "    pickle.dump(y_pred_train, f)\n",
        "\n",
        "with open('y_pred_valid.pkl', 'wb') as f:\n",
        "    pickle.dump(y_pred_valid, f)\n",
        "\n",
        "# Convert raw probabilities to binary predictions using 0.5 threshold\n",
        "y_pred_train_binary = np.round(y_pred_train)  # Threshold at 0.5 for binary classification\n",
        "y_pred_valid_binary = np.round(y_pred_valid)  # Threshold at 0.5 for binary classification\n",
        "\n",
        "# Calculate accuracy for binary predictions\n",
        "train_accuracy = np.mean(y_pred_train_binary == y_train)\n",
        "valid_accuracy = np.mean(y_pred_valid_binary == y_valid)\n",
        "\n",
        "print(f\"Training Accuracy: {train_accuracy:.3f}\")\n",
        "print(f\"Validation Accuracy: {valid_accuracy:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIfU607gKtuo"
      },
      "source": [
        "# Submission\n",
        "\n",
        "Scoring the submission dataset is below, we need to take care of new categories. Then we save the score as a last step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-23T07:57:25.421291Z",
          "iopub.status.busy": "2024-02-23T07:57:25.420595Z",
          "iopub.status.idle": "2024-02-23T07:57:25.431686Z",
          "shell.execute_reply": "2024-02-23T07:57:25.430104Z",
          "shell.execute_reply.started": "2024-02-23T07:57:25.421245Z"
        },
        "id": "WBpc3xuvKtuo",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# # 데이터 타입 일치 여부 확인\n",
        "# dtype_match = X_train.dtypes == X_submission.dtypes\n",
        "\n",
        "# if dtype_match.all():\n",
        "#     print(\"데이터 타입이 일치합니다.\")\n",
        "# else:\n",
        "#     print(\"데이터 타입이 일치하지 않습니다.\")\n",
        "\n",
        "#     # 불일치한 열과 데이터 타입 출력\n",
        "#     mismatched_cols = X_train.columns[~dtype_match].tolist()\n",
        "#     for col in mismatched_cols:\n",
        "#         print(f\"데이터 타입 불일치 - 열: {col}, 훈련 데이터셋: {X_train[col].dtype}, 제출 데이터셋: {X_submission[col].dtype}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-23T08:08:32.930341Z",
          "iopub.status.busy": "2024-02-23T08:08:32.929826Z",
          "iopub.status.idle": "2024-02-23T08:08:32.941404Z",
          "shell.execute_reply": "2024-02-23T08:08:32.940077Z",
          "shell.execute_reply.started": "2024-02-23T08:08:32.930301Z"
        },
        "id": "GZCB_X_aKtuo",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# X_submission = data_submission[cols_pred].to_pandas()\n",
        "# X_submission = convert_strings(X_submission)\n",
        "# categorical_cols = X_train.select_dtypes(include=['category']).columns\n",
        "\n",
        "# for col in categorical_cols:\n",
        "#     train_categories = set(X_train[col].cat.categories)\n",
        "#     submission_categories = set(X_submission[col].cat.categories)\n",
        "#     new_categories = submission_categories - train_categories\n",
        "#     X_submission.loc[X_submission[col].isin(new_categories), col] = \"Unknown\"\n",
        "#     new_dtype = pd.CategoricalDtype(categories=train_categories, ordered=True)\n",
        "#     X_train[col] = X_train[col].astype(new_dtype)\n",
        "#     X_submission[col] = X_submission[col].astype(new_dtype)\n",
        "\n",
        "# y_submission_pred = gbm.predict(X_submission, num_iteration=gbm.best_iteration)\n",
        "\n",
        "\n",
        "# submission = pd.DataFrame({\n",
        "#     \"case_id\": data_submission[\"case_id\"].to_numpy(),\n",
        "#     \"score\": y_submission_pred\n",
        "# }).set_index('case_id')\n",
        "# submission.to_csv(\"./submission.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-23T07:57:27.466675Z",
          "iopub.status.idle": "2024-02-23T07:57:27.467178Z",
          "shell.execute_reply": "2024-02-23T07:57:27.466939Z",
          "shell.execute_reply.started": "2024-02-23T07:57:27.466918Z"
        },
        "id": "D9L9tj7JKtuo",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# # 특성 중요도 가져오기\n",
        "# importances = gbm.feature_importance()\n",
        "\n",
        "# # 중요도를 데이터프레임으로 변환\n",
        "# feature_importances = pd.DataFrame({'feature': X_train.columns, 'importance': importances})\n",
        "\n",
        "# # 중요도에 따라 특성 정렬\n",
        "# feature_importances = feature_importances.sort_values('importance', ascending=False)\n",
        "\n",
        "# # 중요도가 높은 특성 출력\n",
        "# print(feature_importances)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-23T07:57:27.468991Z",
          "iopub.status.idle": "2024-02-23T07:57:27.469505Z",
          "shell.execute_reply": "2024-02-23T07:57:27.469312Z",
          "shell.execute_reply.started": "2024-02-23T07:57:27.469289Z"
        },
        "id": "S66l0zW5Ktuo",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# # 중요도 막대 그래프 그리기\n",
        "# import matplotlib.pyplot as plt\n",
        "# plt.figure(figsize=(15, 15))\n",
        "# plt.title(\"Feature importances\")\n",
        "# plt.barh(feature_importances['feature'], feature_importances['importance'], color='b', align='center')\n",
        "# plt.xlabel(\"Importance\")\n",
        "# plt.ylabel(\"Features\")\n",
        "# plt.gca().invert_yaxis()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-23T07:57:27.471378Z",
          "iopub.status.idle": "2024-02-23T07:57:27.471787Z",
          "shell.execute_reply": "2024-02-23T07:57:27.471611Z",
          "shell.execute_reply.started": "2024-02-23T07:57:27.471592Z"
        },
        "id": "6K0JVGkEKtuo",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# import shap\n",
        "\n",
        "# explainer = shap.Explainer(gbm)\n",
        "# shap_values = explainer.shap_values(X_test)\n",
        "\n",
        "# shap.summary_plot(shap_values, X_test, plot_type='bar')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-23T07:57:27.47313Z",
          "iopub.status.idle": "2024-02-23T07:57:27.47354Z",
          "shell.execute_reply": "2024-02-23T07:57:27.473361Z",
          "shell.execute_reply.started": "2024-02-23T07:57:27.473342Z"
        },
        "id": "u2JoxGL2Ktuo",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# shap.summary_plot(shap_values, X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-23T07:57:27.475142Z",
          "iopub.status.idle": "2024-02-23T07:57:27.47555Z",
          "shell.execute_reply": "2024-02-23T07:57:27.475367Z",
          "shell.execute_reply.started": "2024-02-23T07:57:27.475348Z"
        },
        "id": "hzYr5PNHKtup",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# explanation_object = explainer(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-23T07:57:27.476627Z",
          "iopub.status.idle": "2024-02-23T07:57:27.47737Z",
          "shell.execute_reply": "2024-02-23T07:57:27.477185Z",
          "shell.execute_reply.started": "2024-02-23T07:57:27.477164Z"
        },
        "id": "VerQGFS0Ktup",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# shap.plots.waterfall(explanation_object[0])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "databundleVersionId": 7602123,
          "sourceId": 50160,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 30635,
      "isGpuEnabled": false,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
